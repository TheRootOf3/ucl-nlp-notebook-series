{
 "cells": [
  {
   "source": [
    "# Welcome to the Natural Language Processing (NLP) jupyter notebook series!\n",
    "This mini-course has been prepared with the aim of showing rather practical side of the NLP, than detailed theoretical aspects. This and further notebooks contain brief theoretical introduction to every concept, which is later implemented using Python and popular Python NLP modules. Each notebook also contains practical exercises along with complete solutions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Intro to NLP???\n",
    "\n",
    "read input (1) and process (2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Reading files\n",
    "Usually, natural language processing tasks will be performed on rather large amount of data. Since copy & paste works fine for small articles or paragraphs, we can't really use it when there are thousands of them. Large data sets are stored in files of different formats (e.g. .txt, .csv, .log), which impose a strict rules on how the file is structured. This is important for two reasons in terms of reading files.\n",
    "\n",
    "Firstly, a given file should be interpreted in the same, unambiguous way by all users whether they are people or computer programs. Secondly, the file structure should be corresponding to its content, e.g. if a file contains user tweets it will be more natural to put one tweet per line rather one word per line. By utilizing the fact of how the file is structured, a lot of pre-processing can be ommited by creating an appropriate file loader. \n",
    "\n",
    " A common way of structuring files is by using delimiters like commas ',' or endline characters '\\n'. Very large data sets may be also organized using data bases and their own file systems.\n",
    "\n",
    "### 1.1 Txt files\n",
    "Let's see how to deal with a text file (words.txt) containing 10000 most common English words, one per line. The simplest idea would be to read the whole file and store it as a string. However, by doing so we do not take the advantage of the file structure. With a minimal additional cost instead of one big string, we can create a list containing 10000 separate string elements - words. Let's see how it can be done!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "word_list = []\n",
    "\n",
    "# open 'words.txt' file in a reading 'r' mode.\n",
    "with open('words.txt', mode='r', encoding='utf-8') as f:\n",
    "    word_list = f.read().splitlines()  # split lines based on the endline '\\n' character"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "source": [
    "Now, let's see first 20 words and last 20 words in our list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['a', 'aa', 'aaa', 'aaron', 'ab', 'abandoned', 'abc', 'aberdeen', 'abilities', 'ability', 'able', 'aboriginal', 'abortion', 'about', 'above', 'abraham', 'abroad', 'abs', 'absence', 'absent']\n['zambia', 'zdnet', 'zealand', 'zen', 'zero', 'zimbabwe', 'zinc', 'zip', 'zoloft', 'zone', 'zones', 'zoning', 'zoo', 'zoom', 'zoophilia', 'zope', 'zshops', 'zu', 'zum', 'zus']\n"
     ]
    }
   ],
   "source": [
    "print(word_list[:20]) # First 20 words\n",
    "print(word_list[-20:]) # Last 20 words"
   ]
  },
  {
   "source": [
    "In some cases, we don't want to read the whole file (e.g. when the file is huge). Let's see how to read first 50 words of the words.txt file.\n",
    "Note: a very common mistake when reading files line by line is not removing the endline '\\n' character, which is at the end of each line (see .rstrip() below)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['a', 'aa', 'aaa', 'aaron', 'ab', 'abandoned', 'abc', 'aberdeen', 'abilities', 'ability', 'able', 'aboriginal', 'abortion', 'about', 'above', 'abraham', 'abroad', 'abs', 'absence', 'absent']\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "lines_to_read_num = 50\n",
    "\n",
    "# open 'words.txt' file in a reading 'r' mode.\n",
    "with open('words.txt', mode='r', encoding='utf-8') as f:\n",
    "    for _ in range(lines_to_read_num):  # _ is a wildcard\n",
    "        word_list.append(f.readline().rstrip()) # We want to strip the last character of each line since it is and endline '\\n' character\n",
    "\n",
    "print(word_list[:20]) # First 20 words"
   ]
  },
  {
   "source": [
    "### 1.2 File Encoding \n",
    "Now, let's try to load another text file (japaneseWords.txt) containing 10000 most common Japanese words. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14615/4138467946.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# open 'words.txt' file in a reading 'r' mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'japaneseWords.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mjapanese_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# split lines based on the endline '\\n' character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjapanese_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# First 20 words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "japanese_words = []\n",
    "\n",
    "# open 'words.txt' file in a reading 'r' mode.\n",
    "with open('japaneseWords.txt', mode='r', encoding='utf-8') as f:\n",
    "    japanese_words = f.read().splitlines()  # split lines based on the endline '\\n' character\n",
    "print(japanese_words[:20]) # First 20 words"
   ]
  },
  {
   "source": [
    "Oops, we got the `UnicodeDecodeError`... This error is raised because the default encoding used by Python for reading files (`utf-8`) is different from the one used in the japanese words file (`utf-16`). Even though `utf-8` is widely used and in most cases sufficient, in the future you may deal with data encoded using other encodings (like in this case). Try to solve this problem by approprietly changing the `encoding` parameter in the `open` function and run it again!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.3 CSV files\n",
    "So far, our data set consisted of one word per line without any additional details. However, in many cases each entry in your data set may contain more than one field. For example, imagine that in the english word list, we would like to annotate each word with a name of the lexical categories it belongs to (verb, noun, etc.). In cases like this, csv files are very convenient."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}