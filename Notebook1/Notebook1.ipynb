{
 "cells": [
  {
   "source": [
    "# Welcome to the Natural Language Processing (NLP) jupyter notebook series!\n",
    "This mini-course has been prepared with the aim of showing rather practical side of the NLP, than detailed theoretical aspects. This and further notebooks contain brief theoretical introduction to every concept, which is later implemented using Python and popular Python NLP modules. Each notebook also contains practical exercises along with complete solutions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Intro to NLP???\n",
    "\n",
    "read input (1) and process (2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Reading files\n",
    "Usually, natural language processing tasks will be performed on rather large amount of data. Since copy&paste works fine for small articles or paragraphs, we can't really use it when there are thousands of them. Large data sets are stored in files of different formats (e.g. .txt, .csv, .log), which impose a strict rules on how the file is structured. This is important for two reasons in terms of reading files.\n",
    "\n",
    "Firstly, a given file should be interpreted in the same, unambiguous way by all users whether they are people or computer programs. Secondly, the file structure should be corresponding to its content, e.g. if a file contains user tweets it will be more natural to put one tweet per line rather one word per line. By utilizing the fact of how the file is structured, a lot of pre-processing can be ommited by creating an appropriate file loader. \n",
    "\n",
    " A common way of structuring files is by using delimiters like commas ',' or endline characters '\\n'. Very large data sets may be also organized using data bases and their own file systems.\n",
    "\n",
    "### 1.1 Txt files\n",
    "Let's see how to deal with a text file (words.txt) containing 10000 most common English words, one per line. The simplest idea would be to read the whole file and store it as a string. However, by doing so we do not take the advantage of the file structure. With a minimal additional cost instead of one big string, we can create a list containing 10000 separate string elements - words. Let's see how it can be done!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "word_list = []\n",
    "\n",
    "# open 'words.txt' file in a reading 'r' mode.\n",
    "with open('words.txt', mode='r') as f:\n",
    "    word_list = f.read().splitlines()  # split lines based on the endline '\\n' character"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Now, let's see first 20 words and last 20 words in our list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(word_list[:20]) # First 20 words\n",
    "print(word_list[-20:]) # Last 20 words"
   ]
  },
  {
   "source": [
    "In some cases, we don't want to read the whole file (e.g. when the file is huge). Let's see how to read first 50 words of the words.txt file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "lines_to_read_num = 50\n",
    "\n",
    "# open 'words.txt' file in a reading 'r' mode.\n",
    "with open('words.txt', mode='r') as f:\n",
    "    for _ in range(lines_to_read_num):  # _ is a wildcard\n",
    "        word_list.append(f.readline().rstrip()) # We want to strip the last character of each line since it is and endline '\\n' character\n",
    "\n",
    "print(word_list[:20]) # First 20 words"
   ]
  },
  {
   "source": [
    "TODO: Write about encoding and why it's important\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}