{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Welcome to the Natural Language Processing (NLP) jupyter notebook series!\n",
    "This mini-course has been prepared with the aim of showing rather a practical side of the NLP, than detailed theoretical aspects. This and further notebooks contain a brief theoretical introduction to every concept, which is later implemented using Python and popular Python NLP modules. Each notebook also contains practical exercises along with complete solutions.\n",
    "\n",
    "---\n",
    "\n",
    "# Notebook 1 - Data loading and Regular expressions\n",
    "In this notebook we will cover essentials needed in nearly every NLP project but also very common in other applications:\n",
    "- reading from text files\n",
    "- reading from CSV files using `pandas`\n",
    "- pdf extraction using `PyMuPDF`\n",
    "- regular expressions\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Reading files\n",
    "Usually, Natural Language Processing tasks will be performed on a rather large amount of data. Since copy & paste works fine for small articles or paragraphs, we can't use it when there are thousands of them. Large data sets are stored in files of different formats (e.g. .txt, .csv, .log), which impose a strict rules on how the file is structured. This is important for two reasons in terms of reading files.\n",
    "\n",
    "Firstly, a given file should be interpreted in the same, unambiguous way by all users whether they are people or computer programs. Secondly, the file structure should be corresponding to its content, e.g. if a file contains user tweets it will be more natural to put one tweet per line rather than one word per line. By utilizing the fact of how the file is structured, some of the pre-processing can be omitted by creating an appropriate file loader. \n",
    "\n",
    "A common way of structuring files is by using delimiters like commas ',' or newline characters '\\n'. Very large data sets may be also organized using databases and their file systems.\n",
    "\n",
    "### 1.1. Txt files\n",
    "Let's see how to deal with a text file (words.txt) containing 10000 most common English words, one per line. The simplest idea would be to read the whole file, store it as a string and later extract single words somehow. Let's try it!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "file_string = \"\"\n",
    "\n",
    "''' uncomment if you are running this notebook locally '''\n",
    "# open 'words.txt' file in a reading 'r' mode.\n",
    "# with open('datasets/words.txt', mode='r', encoding='utf-8') as f:\n",
    "#     file_string = f.read()\n",
    "\n",
    "\n",
    "''' uncomment if you are using google colab '''\n",
    "# import urllib\n",
    "# words_file_url = \"https://raw.githubusercontent.com/TheRootOf3/ucl-studentship2021-nlp-notebooks/main/Notebook1/datasets/words.txt\"\n",
    "\n",
    "# with urllib.request.urlopen(words_file_url) as f:\n",
    "#     file_string = f.read().decode('utf-8')\n",
    "\n",
    "\n",
    "file_string[:1000]  # Let's see first 1000 of this string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far, we have loaded the file content to one variable `file_string`. As you can see, all words are joined together and separated with the newline character '\\n'. Now, how can we extract single words and put them into the `word_list`? Let's use the built-in Python `split` method. This is a simple method that splits the source text based on the given delimiter, check out examples below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"I live in the UK\".split(' '))  # delimiter - space character\n",
    "print(\"123-456-789\".split('-')) # delimiter - dash character"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we want to split our text based on the endline character '\\n'. Let's do it and see first 200 words and last 20 words in our list!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_list = file_string.split('\\n')\n",
    "print(word_list[:20]) # First 20 words\n",
    "print(word_list[-20:]) # Last 20 words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nicely done! However, we can achieve the same goal even without the `file_string` variable. This time, we will read the file content directly to a list using the `.splitlines()` method. Let's see how it can be done!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_list_splitlines = []\n",
    "\n",
    "''' uncomment if you are running this notebook locally '''\n",
    "# open 'words.txt' file in a reading 'r' mode.\n",
    "# with open('datasets/words.txt', mode='r', encoding='utf-8') as f:\n",
    "#     word_list_splitlines = f.read().splitlines()  # split lines based on the endline '\\n' character\n",
    "\n",
    "''' uncomment if you are using google colab '''\n",
    "# with urllib.request.urlopen(words_file_url) as f:\n",
    "#     word_list_splitlines = f.read().decode('utf-8').splitlines()  # split lines based on the endline '\\n' character\n",
    "\n",
    "\n",
    "word_list_splitlines[:5]"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In some cases, we don't want to read the whole file. Let's see how to read the first 50 words of the words.txt file.\n",
    "Note: a very common mistake when reading files line by line is not removing the newline '\\n' character, which is at the end of each line (see .rstrip() below)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_list = []\n",
    "lines_to_read_num = 50\n",
    "\n",
    "''' uncomment if you are running this notebook locally '''\n",
    "# open 'words.txt' file in a reading 'r' mode.\n",
    "# with open('datasets/words.txt', mode='r', encoding='utf-8') as f:\n",
    "#     for _ in range(lines_to_read_num):  # _ is a wildcard\n",
    "#         word_list.append(f.readline().rstrip()) # We want to strip the last character of each line since it is and endline '\\n' character\n",
    "\n",
    "''' uncomment if you are using google colab '''\n",
    "# with urllib.request.urlopen(words_file_url) as f:\n",
    "#     for _ in range(lines_to_read_num):  # _ is a wildcard\n",
    "#         word_list.append(f.readline().decode('utf-8').rstrip()) # We want to strip the last character of each line since it is and endline '\\n' character\n",
    "\n",
    "\n",
    "print(word_list[:20]) # First 20 words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2. File Encoding \n",
    "Now, let's try to load another text file (japaneseWords.txt) containing 10000 most common Japanese words. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "japanese_words = []\n",
    "\n",
    "''' uncomment if you are running this notebook locally '''\n",
    "# open 'words.txt' file in a reading 'r' mode.\n",
    "# with open('datasets/japaneseWords.txt', mode='r', encoding='utf-8') as f:\n",
    "#     japanese_words = f.read().splitlines()  # split lines based on the endline '\\n' character\n",
    "\n",
    "''' uncomment if you are using google colab '''\n",
    "# japanese_words_file_url = \"https://raw.githubusercontent.com/TheRootOf3/ucl-studentship2021-nlp-notebooks/main/Notebook1/datasets/japaneseWords.txt\"\n",
    "# with urllib.request.urlopen(japanese_words_file_url) as f:\n",
    "#     japanese_words = f.read().decode('utf-8').splitlines()  # split lines based on the endline '\\n' character\n",
    "\n",
    "\n",
    "print(japanese_words[:20]) # First 20 words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Oops, we got the `UnicodeDecodeError`... This error is raised because the default encoding used by Python for reading files (`utf-8`) is different from the one used in the Japanese words file (`utf-16`). Even though `utf-8` is widely used and in most cases sufficient, in the future you may deal with data encoded using other encodings (like in this case). Try to solve this problem by appropriately changing the `encoding` (or if using google colab `decode`) parameter in the `open` function and run it again!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3. CSV files\n",
    "So far, our data set has consisted of one word per line without any additional details. However, in many cases, each entry in your data set may contain more than one field. For example, imagine that in the English word list, we would like to annotate each word with a name of the lexical categories it belongs to (verb, noun, etc.). In cases like this, CSV files are very convenient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For handling large or multi-column CSV data sets there is a nice Python module called `pandas`. It is a module for conveniently managing big data with multiple features (check the official [module documentation](https://pandas.pydata.org/docs/)). In the beginning, let's import it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's see how we can easily import and present simple dataset containing information about text entries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dummy_dataset_file = \"https://raw.githubusercontent.com/TheRootOf3/ucl-studentship2021-nlp-notebooks/main/Notebook1/datasets/dummy_data.csv\"\n",
    "\n",
    "''' uncomment if you want to run it locally '''\n",
    "# dummy_dataset_file = \"./datasets/dummy_data.csv\" \n",
    "\n",
    "simple_data_set = pd.read_csv(dummy_dataset_file, encoding='utf-8')\n",
    "simple_data_set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see pandas creates a table containing columns and rows. This table object is called `DataFrame` and each column crates a `Series`. Every entry is presented as a row.\n",
    "\n",
    "If you deal with a very large data set you probably don't want to print all entries, you rather want to check if the file loaded properly. To do so, you can apply the .head() method on the newly created DataFrame object."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "simple_data_set.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are not familiar with `pandas`, you can treat it as a more powerful MS Excel, since you can manipulate or process all the data using Python. Let's say you want to see all `text` fields of entries being classified as the `sms`. Additionally, you want these text fields formatted in lower case."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sms_df = simple_data_set[simple_data_set['type'] == 'sms']\n",
    "sms_df['text'].str.lower()  # Equivalent to new_df.text.str.lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's try to add a new column containing the length of the messages in the `sms_df`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This line (creates) a new column and fills it with a length (.str.len()) of every message. \n",
    "sms_df['text_length'] = sms_df.text.str.len()\n",
    "sms_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we did this already, we can try sorting and shuffling this dataframe a bit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Sorting based on text_length\n",
    "sms_df.sort_values('text_length')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Shuffling the sms_df using the sample method\n",
    "# parameter `frac` specifies the fraction of rows to return in the random sample, so frac=1 means return all rows.\n",
    "sms_df.sample(frac=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, pandas is a powerful tool! To learn more about it check out [this official 10-min guide](https://pandas.pydata.org/docs/user_guide/10min.html)!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4. PDF Text extraction\n",
    "Sometimes your data won't be in handy and easy to read formats like CSV or txt. If you want to extract textual information from reports or scientific papers it is almost certain that you will have to deal with PDF files. To read them we will use another Python module called `PyMuPDF`. Here is the complete [documentation](https://pymupdf.readthedocs.io/en/latest/)! \n",
    "\n",
    "**Note: This works only when ran locally**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import fitz  # fitz stands for the PyMuPDF\n",
    "\n",
    "with fitz.open('datasets/hamlet.pdf') as pdfFile:\n",
    "    text = \"\"\n",
    "    for pnum in range(3):  # read first 3 pages of the file\n",
    "        text += pdfFile.load_page(pnum).getText()\n",
    "    print(text)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After reading PDF file content you can either store extracted data to a different file or continue with further operations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Regular Expressions\n",
    "Regular Expressions are very often used in text preprocessing. They are particularly useful for searching in texts when we have a pattern to search for and a corpus of texts to search through. Another application for regular expressions is when there is a pattern which we want to remove from the text or replace.\n",
    "\n",
    "### 2.1. Python re\n",
    "In Python, there is a built-in module for regular expressions called `re`. \n",
    "The simplest regular expression consists of only a text to be matched. Let's see how it works."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "text_sample = \"I like Natural Language Processing! I like dogs.\"\n",
    "re.search(r'like', text_sample)  # search looks for the first match of the pattern in the given source"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Useful Python `re` methods:\n",
    "- match - matches a given pattern at the beginning of the source (often used if you want to match the whole source).\n",
    "- fullmatch - matches if a given pattern matches the whole source.\n",
    "- search - search for a given pattern anywhere in the source (matches only the left-most occurrence).\n",
    "- sub - replaces all occurrences of pattern in the source with a given replacement.\n",
    "- findall - matches all occurrences of a given pattern in the source.\n",
    "\n",
    "For the complete documentation of the `re` module [click here](https://docs.python.org/3/library/re.html)!\n",
    "\n",
    "Let's see how these methods work. Feel free to play with them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "re.sub(r'like', \"don't like\", text_sample)  # sub returns a text with already replaced patterns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(re.match(r'like', \"like\"))\n",
    "print(re.match(r'like', \"like like like\"))\n",
    "print(re.match(r'like', \"I like like like\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# How about this? \n",
    "print(re.match(r'like', \"likelihood\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, it matches much more than we expected. If we want to restrict matching for the only string 'like', we should use another `re` method - fullmatch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(re.fullmatch(r'like', \"likelihood\"))\n",
    "print(re.fullmatch(r'like', \"like\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. Regular expression basics\n",
    "\n",
    "There are several fundamental regular expression patterns presented below. Note: regular expressions are case sensitive ('Cat' is not the same as 'cat').\n",
    "\n",
    "| RE           | Match                    | Example                  |\n",
    "|--------------|--------------------------|--------------------------|\n",
    "| a            | single character `a`     | I like c**a**ts!         |\n",
    "| [abc]        | `a` or `b` or `c`        | I like **c**ats!         |\n",
    "| [^A]         | not uppercase letter `A` | A**t** noon.             |\n",
    "| [Cc]at       | `Cat` or `cat`           | **Cat** is an animal.    |\n",
    "| [1234567890] | Any digit                | My password is **1**234. |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(re.search(r'at', \"cats\"))\n",
    "print(re.search(r'[abc]', \"cats\"))\n",
    "print(re.search(r'[^ Atn]', \"At noon.\"))\n",
    "\n",
    "print(re.sub(r'[Dd]og', \"(DOG)\", \"Dog is my favourite animal, so I adopted a dog.\"))\n",
    "print(re.sub(r'[1234567890]', \"(DIGIT)\", \"I have 2 dogs, 3 cats and $100 in my pocket.\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3. Regular expression ranges\n",
    "How about the regular expression for any uppercase letter? This would look like this: [ABCDEFGHIJKLMNOPQRSTUVWXYZ]. Quite inconvenient, right? For this, we can use a `range` instead.\n",
    "Range equivalent for that regex would be [A-Z] - any uppercase letter from A to Z, so all of them. You can also define other ranges, here are some examples.\n",
    "\n",
    "| RE       | Match                    | Example                      |\n",
    "|----------|--------------------------|------------------------------|\n",
    "| [A-Z]    | any uppercase letter     | **I** like cats!             |\n",
    "| [a-z]    | any lowercase letter     | I **l**ike cats!             |\n",
    "| [0-9]    | any digit                | I live on the **4**th floor. |\n",
    "| [1-4]    | `1` or `2` or `3` or `4` | I have 5**1** years.         | \n",
    "| [a-zA-Z] | any letter               | 1234 **a**bcd                |\n",
    "| [^a-zA-Z] | not a letter            | **1**234 abcd                |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(re.search(r'[A-Z]', \"i like NLP!\"))\n",
    "print(re.search(r'[^ a-zA-Z]', \"I have 2 dogs.\"))  # note that we also negate the space ( ) character\n",
    "print(re.search(r'[^a-zA-Z]', \"I have 2 dogs.\"))\n",
    "print(re.search(r'[5-8]', \"My passwords is 12345678\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4. Regular operators and counters\n",
    "\n",
    "What if we want to express a pattern that contains *one or more* digits? Or how to create a pattern where some part of it may occur but is not mandatory? \n",
    "There are some POWERFUL operators: \n",
    "\n",
    "| Operator | Meaning                  |\n",
    "|----------|--------------------------|\n",
    "| ?        | exactly zero or one occurrence of the previous char or expression                   |\n",
    "| *        | zero or more occurrences of the previous char or expression             |\n",
    "| +        | one or more occurrences of the previous char or expression            |\n",
    "| \\|       | disjunction              |\n",
    "\n",
    "Examples:\n",
    "\n",
    "| RE     | Match                             | Example                   |\n",
    "|--------|-----------------------------------|---------------------------|\n",
    "| dogs?  | dog or dogs                       | I have a **dog**.         |\n",
    "| ab*    | `a` followed by zero or more `b`s | **abbbb** (also **a**)    |\n",
    "| ab+    | `a` followed by one or more `b`s  | **abbbb** (but not a)     |\n",
    "| a(bc)+ | `a` followed by one or more `bc`s | **abcbcbc***              |\n",
    "| ab\\|ba | either `ab` or `ba`               | **ab** is cooler than ba. |\n",
    "\n",
    "**Note that these operators are exhaustive, meaning they will always find the longest possible match. (e.g. if the pattern is ab\\* and the source text abbbb, the match will be abbbb and not ab)** \n",
    "\n",
    "But if those characters have special meaning how do we look for something like `2+2`? If we just use this as a regex it will match all occurrences of `22`, but also `222` and so on... If we want to treat the plus sign as a regular character we have to `escape` it using the backslash `\\` before. So the correct version of our regular expression would look like this: `2\\+2`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(re.search(r'g+', \"Do you like my doggo?\"))\n",
    "print(re.search(r'(ha)+', \"hahahaha\"))\n",
    "print(re.sub(r'[0-9]+', \"NUMBER\", \"I have 2 dogs, 3 cats and $100 in my pocket.\"))\n",
    "\n",
    "# Here we also introduce the 'findall' method which returs all matches.\n",
    "print(re.findall(r'[Dd]ogs?', \"Dogs are very smart animals but my dog exceptional\"))\n",
    "print(re.findall(r'[Dd]ogs?|[Cc]ats?', \"Dogs and cats are the most popular pets. My dog is bigger than my cat.\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5. More advanced operators\n",
    "Now, let's say we want to replace all matches of the word \"like\" with \"don't like\". It shouldn't be a problem, right?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "re.sub(r'like', \"don't like\", \"I like dogs. I like cats. Maximum likelihood estimate (MLE) is a powerful tool!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Oops, we \"like\" in the word \"likelihood\" also has been replaced. How can we prevent this? There are some other special operators which will help us with this!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Operator | Meaning           |\n",
    "|----------|-------------------|\n",
    "| .        | any character     |\n",
    "| \\b       | word boundary     |\n",
    "| \\B       | non-word boundary |\n",
    "| ^        | starts with       |\n",
    "| $        | ends with         |\n",
    "\n",
    "and some convenient shortcuts:\n",
    "\n",
    "| Operator | expansion    | Meaning           | Example                  |\n",
    "|----------|--------------|-------------------|--------------------------|\n",
    "| \\d       | [0-9]        | any digit         | I have **2** dogs.       |\n",
    "| \\D       | [^\\d]        | any non-digit     | **I** have 2 dogs.       |\n",
    "| \\w       | [a-zA-Z0-9_] | any character     | **I** don't have 2 dogs. |\n",
    "| \\W       | [^\\w]        | any non-character | I don**'**t have 2 dogs. |\n",
    "| \\s       | [ \\r\\y\\n\\f]  | whitespace        | I( )have 2 dogs.         |\n",
    "| \\S       | [^\\s]        | Non-whitespace    | **I** have 2 dogs.       |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having all the knowledge, we can build more sophisticated regular expressions. Firstly, let's try to solve our previous problem using word boundaries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "re.sub(r'\\blike\\b', \"don't like\", \"I like dogs. I like cats. Maximum likelihood estimate (MLE) is a powerful tool!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6. Some examples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's see some examples\n",
    "\n",
    "# Compare two quite similar regexes below:\n",
    "print(re.findall(r'male', \"male and female\"))\n",
    "print(re.findall(r'\\bmale\\b', \"male and female\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The use of . wildcard\n",
    "print(re.findall(r'ma.e', \"made, make, male\"))\n",
    "\n",
    "print(re.match(r'.*dog$', \"I like my dog\"))  # Eveything what ends with 'dog' will be accepted\n",
    "print(re.match(r'.*dog$', \"I like my dog.\"))  # Eveything what ends with 'dog' will be accepted\n",
    "print(re.match(r'.*dog\\.$', \"I like my dog.\"))  # Note: Regular ending-sentence dot needs to be escaped using the backslash."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The use of some shortcut operators\n",
    "print(re.findall(r'name is \\w*', \"My name is Andrew and I am 20yo. Her name is Emily!\"))\n",
    "\n",
    "print(re.findall(r'[^\\w\\s]', \"This will find all non-characters like (*) or @#$ without spaces!\"))\n",
    "\n",
    "# date format dd/mm/yyyy\n",
    "print(re.findall(r'\\d\\d/\\d\\d/\\d\\d\\d\\d', \"Today is 10/07/2021. In 10 days there will be 20/07/2021.\"))\n",
    "\n",
    "# UK international phone number formated or without spaces\n",
    "print(re.findall(r'\\+\\d\\d ?\\d\\d\\d\\d ?\\d\\d\\d\\d\\d\\d', \"Call me later, +44 1234 123456 or +441234123456.\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# regular expression matching prize in dollars, pounds or euro\n",
    "\n",
    "matching_dollars_pattern = r'(\\$|€|£)(0|([1-9]\\d*))(\\.\\d*)?'\n",
    "\n",
    "# Thinking about particular examples of what should be considered as 'match' and what shouldn't is helpful when designing the regular expressions\n",
    "positive_dollars_testing = ['$2', '$123', '£1.1', '$0.1', '£0.012', '$1.12', '€0','€0.00']\n",
    "negative_dollars_testing = ['$000', '£012.00', '€012']\n",
    "\n",
    "print(\"Positive tests:\")\n",
    "for test in positive_dollars_testing:\n",
    "    print(re.fullmatch(matching_dollars_pattern, test))\n",
    "\n",
    "print(\"Negative tests:\")\n",
    "for test in negative_dollars_testing:\n",
    "    print(re.fullmatch(matching_dollars_pattern, test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to comprehensively test your regular expressions and learn more about them, there is a great website called [RegExr](https://regexr.com)!"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}