{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 - Naive-Bayes and Logistics Regression in NLP\n",
    "\n",
    "In this notebook, we will make use of the knowledge gained in the previous one and will go over two fundamental classification algorithms in detail. We will cover:\n",
    "- Naive-Bayes classifier fundamentals\n",
    "- NB application in NLP\n",
    "- smoothing\n",
    "\n",
    "and\n",
    "\n",
    "- Logistic Regression fundamentals\n",
    "- LR loss function & regularization\n",
    "- LR learning process \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Naive-Bayes Classifier\n",
    "\n",
    "As stated in the previous notebook, Naive-Bayes is a supervised learning probabilistic classifier. It is based on applying Bayes' probability theorem and using the fact that the occurrence of an event impacts the probability of another event. But how exactly does it work?\n",
    "\n",
    "### 8.1 NB Fundamentals\n",
    "The general purpose of classifiers is to *classify* samples from the dataset into 2 or more **classes**. Since we want to classify text, instead of the term *sample* we will use the term **document**. Thus, classifiers' task is to take an input document *d* and out of all possible classes, return a class *c*, to which the document *d* belongs.\n",
    "\n",
    "Now, since NB is the probabilistic classifier, its role would be to **maximize the probability** of the predicted class c given the input document d.\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq1.png\" alt=\"equation\" width=\"300\"/></div>\n",
    "\n",
    "The intuition of Bayesian classification is to use **Bayes’ rule** to transform the equation above into their probabilities that have some useful properties.\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq2.png\" alt=\"equation\" width=\"300\"/></div>\n",
    "\n",
    "We then substitute the first equation into the second one to get:\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq3.png\" alt=\"equation\" width=\"400\"/></div>\n",
    "\n",
    "We can conveniently simplify the above equation by dropping the denominator *P(d)*. This is possible because we will be computing *P(d|c)P(c) / P(d)* for each possible class, but *P(d)* does not change for each class; we are always asking the most likely class for the same article *d*, which must have the same probability *P(d)*. Thus, we can choose the class that maximizes the simpler formula\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq4.png\" alt=\"equation\" width=\"400\"/></div>\n",
    "\n",
    "Okay, but how do we represent a document *d*? We can represent a document as a set of **features** `d = (f1, f2, f3 ... fn)`. One way to define these features is to use the Bag-of-words model introduced in Notebook 2. After constructing the BOW of the complete dataset, we will be able to express each document as a vector of word counts. Thus, we can treat each vector value associated with a different word as a separate feature giving us information on the words (and optionally their counts) used in the document. Here we also introduce the first of two **simplifying assumptions**: since we use BOW, the **word order doesn't matter**. We don't care about the position of a word in a document.\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq5.png\" alt=\"equation\" width=\"400\"/></div>\n",
    "\n",
    "However, calculating `P(f1, f2, f3 ... fn | c)` requires computing all possible combinations of features (if BOW uses sum pooling than even more!). We need another simplifying assumption called the **naive Bayes assumption** - the **conditional independence between features** given the same class. Hence, we can multiply probabilites as follows:\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq6.png\" alt=\"equation\" width=\"400\"/></div>\n",
    "\n",
    "Resulting in a final equation:\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq8.png\" alt=\"equation\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 NB Training\n",
    "\n",
    "So how do we train the classifier? How does it learn what is *P(c)* and *P(f|c)*? Starting with the first probability, we can simply use frequencies and derive it from the probability definition: the probability of a class in the dataset is the number of documents of this class divided by the total number of all documents. \n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq7.png\" alt=\"equation\" width=\"200\"/></div>\n",
    "\n",
    "Learning the probability of features given a class *P(f<sub>i</sub>|c)* isn't more complicated. We assume a feature is just the existence of a word in the document’s bag of words (set of the vocabulary *V*), and so we’ll want *P(w|c)*, which we compute as **fraction of times the word w<sub>i</sub> appears among all words in all documents of topic c**.\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq9.png\" alt=\"equation\" width=\"400\"/></div>\n",
    "\n",
    "Let's consider the following example:\n",
    "\n",
    "This is our training data:\n",
    "\n",
    "| **Text** | **Labels** |\n",
    "|----------|-----------|\n",
    "|\"What a great match\" | sports |\n",
    "|\"The election results will be out tomorrow\"| not sports |\n",
    "|\"The match was very boring\"| sports |\n",
    "|\"It was a close election\"| not sports |\n",
    "\n",
    "To make the example easier to follow, let’s assume we applied some pre-processing to the sentences and removed stopwords. The resulting sentences are:\n",
    "\n",
    "| **Text** | **Labels** |\n",
    "|----------|------------|\n",
    "|\"great match\"|  sports |\n",
    "|\"election results tomorrow\"| not sports |\n",
    "|\"match boring\"| sports |\n",
    "|\"close election\"| not sports |\n",
    "\n",
    "In our small corpus, we have 2 classes each having 2 senteces. Hence, the probability of each class is:\n",
    "\n",
    "        P(\"sports\") = 2/4 = 0.5\n",
    "        P(\"not sports\") = 2/4 = 0.5\n",
    "\n",
    "Total unique features (words) for \"sports\": 3\n",
    "Total unique features (words) for \"not sports\": 4\n",
    "\n",
    "Let's say we want to assign a class to the following sentence \"that was a very close, great match\". After stop word removal it is \"**close great match**\". Now we need to perform some calculations:\n",
    "\n",
    "1. Likelihood P(\"close great match\"|sports) = P(\"close\"|sports) * P(\"great\"|sports) * P(\"match\"|sports)\n",
    "2. Likelihood P(\"close great match\"|not sports) = P(\"close\"|not sports) * P(\"great\"|not sports) * P(\"match\"|not sports)\n",
    "\n",
    "| word | P(word\\|sports) | P(word\\|not sports)|\n",
    "|------|----------------|-------------------|\n",
    "| close | 0/3 | 1/4 |\n",
    "| great | 1/3| 0/4|\n",
    "| match | 2/3| 0/4|\n",
    "\n",
    "We suspect that the correct class is \"sport\", right? Let's see what happens with the likelihood:\n",
    "P(\"close great match\"|sports) = 0/3 * 1/3 * 2/3\n",
    "P(\"close great match\"|sports) = 0\n",
    "\n",
    "Oops... This example shows a very common situation - there were no training documents classified as \"sports\" containing the word \"close\". As a result, P(\"close\"|\"sports\") results in a painful zero, which also makes the product of probabilities equals 0. We can solve this issue using **smoothing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 NB Smoothing & Unknown words\n",
    "\n",
    "\n",
    "#### Laplace smoothing\n",
    "\n",
    "Smoothing is used to avoid a situation that the classifier assigns zero probability to the whole document (as we can see above). This happens when a classifier sees a word, which IS present in the vocabulary (perhaps in a document of a different class), but it wasn't used in the given context. The intuition behind the smoothing is that we don't want the classifier to assign zero probabilities to previously unseen events - the fact that something wasn't present in the training data, doesn't guarantee that it is impossible.\n",
    "\n",
    "There are many smoothing algorithms but the simplest one is called **Laplace smoothing** or **Add-one smoothing**. The part of the classification algorithm which causes a problem is the probability of a feature given a class *P(f<sub>i</sub>|c)*, which we interpret as the **fraction of times the word w<sub>i</sub> appears among all words in all documents of topic c**. If the numerator is equal to 0, the whole probability is also equal to 0. Laplace smoothing adds 1 to each count resulting in a new formula for the probability. Note that since we artificially increment the number of occurrences of each word in the numerator and denominator, we add the size of the vocabulary |V| to the denominator. This results in the equation:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq10.png\" alt=\"equation\" width=\"600\"/></div>\n",
    "\n",
    "#### Add-k smoothing\n",
    "\n",
    "Add-one smoothing is not the only solution, and very often not the best one. Remember, that the probability mass is finite, which means that if we add some probability to one event, we have to remove it (preferably uniformly) from other events. Adding 1 to each word count, sometimes results in moving too much probability mass from rarely seen to totally unseen events. How can we deal with this situation? Well, the simplest solution is to add a smaller number than 1 to each word count (we don't associate this value with the word count anymore). \n",
    "\n",
    "This smoothing is called **Add-k smoothing** since it adds **k** to each word count. This, however, requires estimating what is the best value for **k**. It can be very different for different datasets and applications, so it should be adjusted to the problem.\n",
    "\n",
    "#### Unseen words\n",
    "\n",
    "How about previously unseen words in none of the contexts (classes)? For example what happens if we try to test the classifier above on a document \"*close funny match*\"? Well, since we use the bag-of-words model, we have already seen (in Notebook 2) that this issue is unsolvable in a simple way because we would have to modify all one-hot vectors. The only reasonable solution, in this case, is to **remove all previously unseen words**. Because of this, NB classifiers need rather big training datasets to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Completed NB example & Implementation\n",
    "\n",
    "Ok, so let's complete our example using the Laplace smoothing.\n",
    "\n",
    "The size of our vocabulary |V| = 7, so we will add it to all denominators.\n",
    "\n",
    "| word | P(word\\|sports) | P(word\\|not sports)|\n",
    "|------|----------------|-------------------|\n",
    "| close | 1/10 | 2/11 |\n",
    "| great | 2/10| 1/11|\n",
    "| match | 3/10| 1/11|\n",
    "\n",
    "P(\"close great match\"|sports) = 1/10 * 2/10 * 3/10 = 0.006\n",
    "P(\"close great match\"|not sports) = 2/11 * 1/11 * 1/11 = 0.0015\n",
    "\n",
    "Now, we have to multiply each probability by the probability of a class (posterior = likelihood * prior), which results in:\n",
    "P(\"close great match\"|sports)*P(sports) = 0.006 * 0.5 = 0.003\n",
    "P(\"close great match\"|not sports)*P(not sports) = 0.0015 * 0.5 = 0.00075\n",
    "\n",
    "Thus, there is a higher probability that the test document is indeed about sports and this would be the decision of the NB classifier.\n",
    "\n",
    "Now, let's implement the same example using `Python` and `scikit-learn`! Firstly let's create a training set and a test document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - sports, 0 - not sports\n",
    "training_corpus = [\"What a great match\",\n",
    "          \"The election results will be out tomorrow\",\n",
    "          \"The match was very boring\",\n",
    "          \"It was a close election\",\n",
    "          ]\n",
    "training_labels = [1, 0, 1, 0]\n",
    "\n",
    "test_doc = \"that was a very close, great match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to preprocess and vectorize documents to extract features (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boring', 'close', 'election', 'great', 'match', 'results', 'tomorrow']\n",
      "[[0 0 0 1 1 0 0]\n",
      " [0 0 1 0 0 1 1]\n",
      " [1 0 0 0 1 0 0]\n",
      " [0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate countvectorizer. You can sepcify what kind of preprocessing will be done by the CountVectorizer\n",
    "# In this case we want all text in lowercase, remove stopwords, keep only alphanumeric characters.\n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "\n",
    "# Fit training data\n",
    "training_data = count_vector.fit_transform(training_corpus)\n",
    "# Let's inspect vectors\n",
    "print(count_vector.get_feature_names())\n",
    "print(training_data.toarray())\n",
    "\n",
    "# transform test data\n",
    "test_doc_transform = count_vector.transform([test_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After vectorizing, let's create and train the NB classifier. For numerical data, we have used a Gaussian NB classifier. For the NLP applications, we will use the Multinomial version of this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the classifier object and fit data. The classifier object is by default created with the Laplace smoothing\n",
    "naive_bayes = MultinomialNB(alpha=1)  # alpha parameter specifies the k number from the add-k smoothing. 1 is the default value\n",
    "naive_bayes.fit(training_data, training_labels)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = naive_bayes.predict(test_doc_transform)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of 1, means the classifier predicts that the test sentence was from the \"sport\" category. Now, let's try with this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc2 = \"that was a very close, great election\"\n",
    "test_doc2_transform = count_vector.transform([test_doc2])\n",
    "predictions = naive_bayes.predict(test_doc2_transform)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[[0.20429777 0.79570223]]\n",
      "[[0.6979549 0.3020451]]\n"
     ]
    }
   ],
   "source": [
    "# We can also inspect calculated probabilites.\n",
    "print(naive_bayes.classes_)  # print the order of classes\n",
    "print(naive_bayes.predict_proba(test_doc_transform))\n",
    "print(naive_bayes.predict_proba(test_doc2_transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just a simple example to familiarize with the NB classifier. Let's look at a real word scenario!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 News classification using NB\n",
    "\n",
    "We’ll use a public dataset from the BBC comprised of 2225 articles, each labeled under one of 5 categories: business, entertainment, politics, sport, or tech. Articles have been already made lowercase and cleaned from punctuation.\n",
    "The goal is to train the classifier to predict what is the class of a given article. Let's start with loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#bbc_dataset_file = \"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/datasets/bbc-text.csv\"\n",
    "\n",
    "''' uncomment if you want to run it locally '''\n",
    "bbc_dataset_file = \"./datasets/bbc-text.csv\"\n",
    "\n",
    "bbc_data = pd.read_csv(bbc_dataset_file)\n",
    "bbc_data = bbc_data[['category', 'text']]\n",
    "bbc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop a well-working classifier it is important to know the structure of the dataset. Let's see what is the size of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bbc_data['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the dataset seems to be balanced. With this sort of data, imbalance in the dataset wouldn't be that problematic, but read this article if you are interested in when imbalanced data can become a problem: [https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5](https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5)\n",
    "\n",
    "Now, let's vectorize documents and train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedShuffleSplit,\n",
    "    cross_val_score)\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    bbc_data['text'],\n",
    "    bbc_data['category'],\n",
    "    test_size=0.2,\n",
    "    random_state=50,\n",
    ")\n",
    "\n",
    "#instantiate countvectorizer \n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "\n",
    "#fit training data\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "#transform test data\n",
    "testing_data = count_vector.transform(X_test)\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data,y_train)\n",
    "\n",
    "predictions = naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9752808988764045\n",
      "f1: 0.9748625696012996\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print('f1: {}'.format(f1_score(y_test, predictions, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - that’s pretty good. But maybe we were lucky and got an \"easy\" 20% test set. Let's use cross-validation to exclude that possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9734831460674158"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "\n",
    "X_whole = CV_count_vector.fit_transform(bbc_data[\"text\"])\n",
    "# CV in sklearn needs lables in numerical values, so let's use LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(bbc_data[\"category\"])\n",
    "scores = cross_val_score(naive_bayes, X_whole, y_enc, cv=StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=100), scoring='accuracy')  # other scoring metric: f1_macro, f1_micro\n",
    "# Mean accuracy scoring \n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a pretty decent score! Let's manually see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_article = X_train[0]\n",
    "business_article = X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_article[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_article[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is the predicted class for the `tech_article`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tech']\n"
     ]
    }
   ],
   "source": [
    "print(naive_bayes.predict(count_vector.transform([tech_article])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 1.364113173456053e-204,\n",
       " 'entertainment': 1.3830745333420627e-177,\n",
       " 'politics': 3.9193233372349485e-209,\n",
       " 'sport': 3.003253362804598e-262,\n",
       " 'tech': 1.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And let's inspect probabilities assigned to all of the classes\n",
    "dict(zip(naive_bayes.classes_, *naive_bayes.predict_proba(count_vector.transform([tech_article]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the `business_article`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business']\n"
     ]
    }
   ],
   "source": [
    "print(naive_bayes.predict(count_vector.transform([business_article])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 1.0,\n",
       " 'entertainment': 1.8608928735341935e-89,\n",
       " 'politics': 7.83023971567479e-69,\n",
       " 'sport': 1.7129276449338016e-105,\n",
       " 'tech': 3.1934048837492243e-86}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And let's inspect probabilities assigned to all of the classes\n",
    "dict(zip(naive_bayes.classes_, *naive_bayes.predict_proba(count_vector.transform([business_article]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Logistic Regression Classifier\n",
    "\n",
    "### 9.1 LR Fundamentals\n",
    "\n",
    "\n",
    "Logistic Regression is the another supervised learning algorithm and probabilistic classifier. This means that it will be also interested in calculating the probability *P(c|d)*:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq1.png\" alt=\"equation\" width=\"300\"/></div>\n",
    "\n",
    "NB classifier used the Bayes' theorem and additional assumptions to derive another equation from the one above. NB uses the probability of a document as a set of features given a class *P(d|c)*, which was learned from the training dataset. \n",
    "\n",
    "Logistic regression does it differently. As a discriminative classifier, it tries to learn differences between classes rather than how class representatives \"look like\". Thus, it will directly try to learn the probability of a class given a set of features representing a document *P(c|d)*. The key of this algorithm is in determining which features discriminate documents most efficiently, by assigning these features appropriate **weights** (parameters). For example, in the \"sports\" or \"not sports\" text classification, the word \"football\" will probably\n",
    "have strong positive weight and the word \"princess\", probably strong negative weight. \n",
    "\n",
    "In supervised machine learning, we have input features and sets of labels. To make predictions based on data, we use a function F with some parameters Θ to map features to output labels. To get an optimum mapping from features to labels, we have to minimize the cost function, which works by comparing how closely the output Ŷ is to the true labels Y from the training data. Learning takes place by updating the parameters and repeating the process until the cost is minimized.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/sup_learning.jpeg\" alt=\"equation\" width=\"600\"/></div>\n",
    "\n",
    "Parts of the Logistic Regression algorithm:\n",
    "1. Feature representation - in our case using one-hot vectors, but in general, using **word embeddings** (Notebook 5)\n",
    "2. A classification function that gives the estimated class using *P(c|d)* - we will use two most popular ones - **sigmoid** (binomial LR) and **softmax** (multinomial LR)\n",
    "3. The cost function (or loss function) used for learning, which tells us the difference between the estimated class ĉ and the true class c. We will use **cross-entropy loss function**\n",
    "4. An algorithm for optimizing the classifier by adjusting parameters and consequently minimizing the cost function. One of them is the **Stochastic Gradient Decent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we represent features using a vector `x=[x`<sub>`1`</sub>`, x`<sub>`2`</sub>`, x`<sub>`3`</sub>` ... x`<sub>`n`</sub>`]` and we need to associate some weights with each of these features, we can also represent these weights using a **weights vector** `w=[w`<sub>`1`</sub>`, w`<sub>`2`</sub>`, w`<sub>`3`</sub>` ... w`<sub>`n`</sub>`]`. Then, we can calculate the score for the test document by multiplying each feature with the associated weight and adding all results together. There is also another parameter - the **bias term** `b`, which is added to the sum.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq11.png\" alt=\"equation\" width=\"300\"/></div>\n",
    "\n",
    "In other words, we want the sum of the **dot product** of the features and weights vectors and the bias term:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq12.png\" alt=\"equation\" width=\"300\"/></div>\n",
    "\n",
    "But how do we map this score to the probability, since the score can be any real number from −∞ to ∞? We will use a **sigmoid function** *σ(z)*, which is also called the logistic function.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq13.png\" alt=\"equation\" width=\"700\"/></div>\n",
    "\n",
    "As you can see, it is symmetric with respect to the point (0, 0.5) and it maps all real numbers to the range (0,1) - this is exactly what we need! For two classes (binomial logistic regression), class 0 and class 1, we have probabilities as follows:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq14.png\" alt=\"equation\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "Okay, so how does the classifier learn the correct weights vector and the bias? It starts with some arbitrary values, which are then used to classify a training set. Each result is compared with the true label using the loss functions, in our case with the cross-entropy loss function. This function tells the classifier what is the difference between the classifier output and the true label. Taking into account loss, the classifier needs to adjust its parameters *θ* (in our case *θ = w, b*) to minimize the loss. This is equivalent to finding parameters *θ*, that minimize the loss function (find its minimum). \n",
    "\n",
    "#### Minimizing the loss\n",
    "\n",
    "Since the loss function is parameterized by weights and the bias term, finding its minimum is a multidimensional task. The popular algorithm for finding the minimum of a function is the Stochastic Gradient Decent (SGD). Gradient Descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters *θ*) the function’s slope is rising the most steeply, and moving in the opposite direction. The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself 360 degrees, find the direction where the ground is sloping the steepest, and walk downhill in that direction.\n",
    "\n",
    "I strongly encourage you to read [this chapter](https://web.stanford.edu/~jurafsky/slp3/5.pdf) of the book “Speech and Language Processing” by Daniel Jurafsky and James H. Martin as it explains the idea behind SGD and Logistic Regression very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "\n",
    "There is still at least one more thing to consider. What happens if we choose weights that make the model perfectly match the training data? This may result in overfitting - the same model may have much worse performance on the testing data. To avoid it we can add a special **regularization term**, which penalizes large weights. The reason for penalizing large weights is that they cause some features (present only in the training set) to be much more important than others. When a model is exposed to unseen data, this may block out other crucial features and their combinations.\n",
    "\n",
    "Having all this knowledge, let's try to implement a Logistic Regression classifier on the BBC dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 LR example using the bbc news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_data = pd.read_csv(bbc_dataset_file)\n",
    "bbc_data = bbc_data[['category', 'text']]\n",
    "bbc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vectorize words, split dataset into training and testing and fit the classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate countvectorizer \n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    bbc_data['text'],\n",
    "    bbc_data['category'],\n",
    "    test_size=0.2,\n",
    "    random_state=50,\n",
    ")\n",
    "\n",
    "# remember to fit count vectorizer on the training data, because only these words are present in the training set. \n",
    "# Adding words from the testing set makes this set no longer \"unseen\" to the model\n",
    "X_train_trans = count_vector.fit_transform(X_train)\n",
    "y_train_trans = le.fit_transform(y_train)\n",
    "\n",
    "# Logistic Regression classifier has several parameters including the penalty for big weights (l1, l2) and the method of finding the loss function minimum (solver)\n",
    "lr = LogisticRegression(random_state=0, penalty='l2', solver='lbfgs', verbose=0)\n",
    "lr.fit(X_train_trans, y_train_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9617977528089887"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_trans = count_vector.transform(X_test)\n",
    "y_test_trans = le.transform(y_test)\n",
    "\n",
    "predictions = lr.predict(X_test_trans)\n",
    "accuracy_score(y_test_trans, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's use CV to exclude \"lucky\" test set. It may take more time than usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.969438202247191"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV in sklearn needs lables in numerical values, so let's use LabelEncoder\n",
    "CV_count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "X_whole = CV_count_vector.fit_transform(bbc_data[\"text\"])\n",
    "y_whole = le.transform(bbc_data[\"category\"])\n",
    "\n",
    "scores = cross_val_score(lr, X_whole, y_whole, cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0), scoring='accuracy')  # other scoring metric: f1_macro, f1_micro\n",
    "# Mean accuracy scoring \n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a simple example of how the classifier performs on a tech article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_article = X_train[0]\n",
    "tech_article[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 2.212367750931827e-08,\n",
       " 'entertainment': 0.00014193231439807016,\n",
       " 'politics': 6.029509334365114e-07,\n",
       " 'sport': 3.55322773201083e-09,\n",
       " 'tech': 0.9998574390577633}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(le.inverse_transform(lr.classes_), *lr.predict_proba(count_vector.transform([tech_article]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing we can do is to inspect which words have the most positive and most negative weights (are influencing the classification most). The `lr.coef_` gives us weights trained by the classifier for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.77008159e-02 -4.31583235e-03  4.22547508e-02 ... -5.44355983e-04\n",
      "  -2.10171741e-08 -2.09024382e-04]\n",
      " [-6.02582168e-02  6.18273670e-03  6.68598754e-02 ...  6.64317897e-03\n",
      "  -1.03485585e-05  2.50823664e-03]\n",
      " [-2.50935813e-02 -6.19608971e-05  2.08950764e-02 ... -8.69366923e-04\n",
      "  -5.50843459e-07 -4.68280347e-05]\n",
      " [ 1.05872546e-01 -5.34020718e-05 -1.16065069e-01 ... -4.88809180e-03\n",
      "   1.25195121e-05 -1.61260505e-03]\n",
      " [-3.82215641e-02 -1.75154138e-03 -1.39446338e-02 ... -3.41364259e-04\n",
      "  -1.59909299e-06 -6.39779168e-04]]\n",
      "Shape: (5, 25597)\n"
     ]
    }
   ],
   "source": [
    "print(lr.coef_)\n",
    "print(\"Shape: {}\".format(np.shape(lr.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each class has separate weights (there are 5 classes and 5 dimensions), as this is a Multinomial Logistic Regression classification task. Let's see the top-weighted words for each class. Firstly, we need to associate classes names with values, then we can see which words discriminate classes in the best way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business' 'entertainment' 'politics' 'sport' 'tech']\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Classes names are mapped to numbers (we used the label encoder for this, so let's see what is the order)\n",
    "class_names = le.inverse_transform(lr.classes_)\n",
    "print(class_names)\n",
    "print(lr.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = {}\n",
    "weights_dict[\"feature\"] = count_vector.get_feature_names()\n",
    "for class_num in range(5):\n",
    "    weights_dict[class_names[class_num]] = lr.coef_[class_num]\n",
    "\n",
    "bbc_features_weights = pd.DataFrame(weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9518</th>\n",
       "      <td>firm</td>\n",
       "      <td>0.571839</td>\n",
       "      <td>-0.288587</td>\n",
       "      <td>-0.082418</td>\n",
       "      <td>-0.181756</td>\n",
       "      <td>-0.019078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>euros</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>-0.152402</td>\n",
       "      <td>-0.064496</td>\n",
       "      <td>-0.149050</td>\n",
       "      <td>-0.099961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20804</th>\n",
       "      <td>shares</td>\n",
       "      <td>0.384076</td>\n",
       "      <td>-0.132941</td>\n",
       "      <td>-0.037758</td>\n",
       "      <td>-0.124302</td>\n",
       "      <td>-0.089076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>bank</td>\n",
       "      <td>0.360531</td>\n",
       "      <td>-0.113132</td>\n",
       "      <td>-0.065330</td>\n",
       "      <td>-0.109061</td>\n",
       "      <td>-0.073009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18146</th>\n",
       "      <td>profits</td>\n",
       "      <td>0.340768</td>\n",
       "      <td>-0.084489</td>\n",
       "      <td>-0.039651</td>\n",
       "      <td>-0.103981</td>\n",
       "      <td>-0.112647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>britain</td>\n",
       "      <td>-0.201828</td>\n",
       "      <td>-0.062542</td>\n",
       "      <td>0.246412</td>\n",
       "      <td>0.072051</td>\n",
       "      <td>-0.054093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25144</th>\n",
       "      <td>win</td>\n",
       "      <td>-0.204785</td>\n",
       "      <td>-0.071892</td>\n",
       "      <td>-0.033466</td>\n",
       "      <td>0.399809</td>\n",
       "      <td>-0.089665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16365</th>\n",
       "      <td>old</td>\n",
       "      <td>-0.213929</td>\n",
       "      <td>0.015799</td>\n",
       "      <td>-0.030318</td>\n",
       "      <td>0.211419</td>\n",
       "      <td>0.017030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>film</td>\n",
       "      <td>-0.249245</td>\n",
       "      <td>0.567778</td>\n",
       "      <td>-0.145280</td>\n",
       "      <td>-0.196645</td>\n",
       "      <td>0.023392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>163</td>\n",
       "      <td>-0.448092</td>\n",
       "      <td>0.645419</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>-0.100412</td>\n",
       "      <td>-0.069067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25597 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature  business  entertainment  politics     sport      tech\n",
       "9518      firm  0.571839      -0.288587 -0.082418 -0.181756 -0.019078\n",
       "8757     euros  0.465909      -0.152402 -0.064496 -0.149050 -0.099961\n",
       "20804   shares  0.384076      -0.132941 -0.037758 -0.124302 -0.089076\n",
       "2955      bank  0.360531      -0.113132 -0.065330 -0.109061 -0.073009\n",
       "18146  profits  0.340768      -0.084489 -0.039651 -0.103981 -0.112647\n",
       "...        ...       ...            ...       ...       ...       ...\n",
       "4001   britain -0.201828      -0.062542  0.246412  0.072051 -0.054093\n",
       "25144      win -0.204785      -0.071892 -0.033466  0.399809 -0.089665\n",
       "16365      old -0.213929       0.015799 -0.030318  0.211419  0.017030\n",
       "9433      film -0.249245       0.567778 -0.145280 -0.196645  0.023392\n",
       "225        163 -0.448092       0.645419 -0.027848 -0.100412 -0.069067\n",
       "\n",
       "[25597 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_features_weights.sort_values(by=\"business\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>163</td>\n",
       "      <td>-0.448092</td>\n",
       "      <td>0.645419</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>-0.100412</td>\n",
       "      <td>-0.069067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>film</td>\n",
       "      <td>-0.249245</td>\n",
       "      <td>0.567778</td>\n",
       "      <td>-0.145280</td>\n",
       "      <td>-0.196645</td>\n",
       "      <td>0.023392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21128</th>\n",
       "      <td>singer</td>\n",
       "      <td>-0.121295</td>\n",
       "      <td>0.393541</td>\n",
       "      <td>-0.076322</td>\n",
       "      <td>-0.119649</td>\n",
       "      <td>-0.076276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23798</th>\n",
       "      <td>tv</td>\n",
       "      <td>-0.124427</td>\n",
       "      <td>0.353531</td>\n",
       "      <td>-0.144471</td>\n",
       "      <td>-0.165795</td>\n",
       "      <td>0.081160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15650</th>\n",
       "      <td>music</td>\n",
       "      <td>-0.201312</td>\n",
       "      <td>0.352136</td>\n",
       "      <td>-0.126266</td>\n",
       "      <td>-0.192007</td>\n",
       "      <td>0.167450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22913</th>\n",
       "      <td>technology</td>\n",
       "      <td>-0.201781</td>\n",
       "      <td>-0.176289</td>\n",
       "      <td>-0.056054</td>\n",
       "      <td>-0.117837</td>\n",
       "      <td>0.551962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10150</th>\n",
       "      <td>games</td>\n",
       "      <td>-0.088289</td>\n",
       "      <td>-0.196881</td>\n",
       "      <td>-0.022945</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.297390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10139</th>\n",
       "      <td>game</td>\n",
       "      <td>-0.188651</td>\n",
       "      <td>-0.220367</td>\n",
       "      <td>-0.061090</td>\n",
       "      <td>0.196689</td>\n",
       "      <td>0.273420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10599</th>\n",
       "      <td>government</td>\n",
       "      <td>0.195998</td>\n",
       "      <td>-0.244397</td>\n",
       "      <td>0.349831</td>\n",
       "      <td>-0.150857</td>\n",
       "      <td>-0.150576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9518</th>\n",
       "      <td>firm</td>\n",
       "      <td>0.571839</td>\n",
       "      <td>-0.288587</td>\n",
       "      <td>-0.082418</td>\n",
       "      <td>-0.181756</td>\n",
       "      <td>-0.019078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25597 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  business  entertainment  politics     sport      tech\n",
       "225           163 -0.448092       0.645419 -0.027848 -0.100412 -0.069067\n",
       "9433         film -0.249245       0.567778 -0.145280 -0.196645  0.023392\n",
       "21128      singer -0.121295       0.393541 -0.076322 -0.119649 -0.076276\n",
       "23798          tv -0.124427       0.353531 -0.144471 -0.165795  0.081160\n",
       "15650       music -0.201312       0.352136 -0.126266 -0.192007  0.167450\n",
       "...           ...       ...            ...       ...       ...       ...\n",
       "22913  technology -0.201781      -0.176289 -0.056054 -0.117837  0.551962\n",
       "10150       games -0.088289      -0.196881 -0.022945  0.010725  0.297390\n",
       "10139        game -0.188651      -0.220367 -0.061090  0.196689  0.273420\n",
       "10599  government  0.195998      -0.244397  0.349831 -0.150857 -0.150576\n",
       "9518         firm  0.571839      -0.288587 -0.082418 -0.181756 -0.019078\n",
       "\n",
       "[25597 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_features_weights.sort_values(by=\"entertainment\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16982</th>\n",
       "      <td>party</td>\n",
       "      <td>-0.173311</td>\n",
       "      <td>-0.139301</td>\n",
       "      <td>0.478468</td>\n",
       "      <td>-0.099592</td>\n",
       "      <td>-0.066263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>blair</td>\n",
       "      <td>-0.163510</td>\n",
       "      <td>-0.110918</td>\n",
       "      <td>0.384272</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>-0.064248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10599</th>\n",
       "      <td>government</td>\n",
       "      <td>0.195998</td>\n",
       "      <td>-0.244397</td>\n",
       "      <td>0.349831</td>\n",
       "      <td>-0.150857</td>\n",
       "      <td>-0.150576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13475</th>\n",
       "      <td>labour</td>\n",
       "      <td>-0.091466</td>\n",
       "      <td>-0.108259</td>\n",
       "      <td>0.331888</td>\n",
       "      <td>-0.083972</td>\n",
       "      <td>-0.048192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8242</th>\n",
       "      <td>election</td>\n",
       "      <td>-0.084945</td>\n",
       "      <td>-0.121483</td>\n",
       "      <td>0.325777</td>\n",
       "      <td>-0.067464</td>\n",
       "      <td>-0.051885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>balls</td>\n",
       "      <td>0.127090</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>-0.125880</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15650</th>\n",
       "      <td>music</td>\n",
       "      <td>-0.201312</td>\n",
       "      <td>0.352136</td>\n",
       "      <td>-0.126266</td>\n",
       "      <td>-0.192007</td>\n",
       "      <td>0.167450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>companies</td>\n",
       "      <td>0.215547</td>\n",
       "      <td>-0.036713</td>\n",
       "      <td>-0.133618</td>\n",
       "      <td>-0.086322</td>\n",
       "      <td>0.041106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23798</th>\n",
       "      <td>tv</td>\n",
       "      <td>-0.124427</td>\n",
       "      <td>0.353531</td>\n",
       "      <td>-0.144471</td>\n",
       "      <td>-0.165795</td>\n",
       "      <td>0.081160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>film</td>\n",
       "      <td>-0.249245</td>\n",
       "      <td>0.567778</td>\n",
       "      <td>-0.145280</td>\n",
       "      <td>-0.196645</td>\n",
       "      <td>0.023392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25597 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  business  entertainment  politics     sport      tech\n",
       "16982       party -0.173311      -0.139301  0.478468 -0.099592 -0.066263\n",
       "3495        blair -0.163510      -0.110918  0.384272 -0.045596 -0.064248\n",
       "10599  government  0.195998      -0.244397  0.349831 -0.150857 -0.150576\n",
       "13475      labour -0.091466      -0.108259  0.331888 -0.083972 -0.048192\n",
       "8242     election -0.084945      -0.121483  0.325777 -0.067464 -0.051885\n",
       "...           ...       ...            ...       ...       ...       ...\n",
       "2921        balls  0.127090      -0.000990 -0.125880 -0.000093 -0.000127\n",
       "15650       music -0.201312       0.352136 -0.126266 -0.192007  0.167450\n",
       "5560    companies  0.215547      -0.036713 -0.133618 -0.086322  0.041106\n",
       "23798          tv -0.124427       0.353531 -0.144471 -0.165795  0.081160\n",
       "9433         film -0.249245       0.567778 -0.145280 -0.196645  0.023392\n",
       "\n",
       "[25597 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_features_weights.sort_values(by=\"politics\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25144</th>\n",
       "      <td>win</td>\n",
       "      <td>-0.204785</td>\n",
       "      <td>-0.071892</td>\n",
       "      <td>-0.033466</td>\n",
       "      <td>0.399809</td>\n",
       "      <td>-0.089665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14700</th>\n",
       "      <td>match</td>\n",
       "      <td>-0.166496</td>\n",
       "      <td>-0.127954</td>\n",
       "      <td>-0.040573</td>\n",
       "      <td>0.396486</td>\n",
       "      <td>-0.061463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22890</th>\n",
       "      <td>team</td>\n",
       "      <td>-0.107919</td>\n",
       "      <td>-0.126025</td>\n",
       "      <td>-0.096541</td>\n",
       "      <td>0.361067</td>\n",
       "      <td>-0.030583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>cup</td>\n",
       "      <td>-0.137345</td>\n",
       "      <td>-0.090435</td>\n",
       "      <td>-0.037199</td>\n",
       "      <td>0.336211</td>\n",
       "      <td>-0.071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12311</th>\n",
       "      <td>injury</td>\n",
       "      <td>-0.122379</td>\n",
       "      <td>-0.078572</td>\n",
       "      <td>-0.049308</td>\n",
       "      <td>0.290245</td>\n",
       "      <td>-0.039987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23860</th>\n",
       "      <td>uk</td>\n",
       "      <td>-0.102038</td>\n",
       "      <td>0.116291</td>\n",
       "      <td>0.206630</td>\n",
       "      <td>-0.170610</td>\n",
       "      <td>-0.050274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9518</th>\n",
       "      <td>firm</td>\n",
       "      <td>0.571839</td>\n",
       "      <td>-0.288587</td>\n",
       "      <td>-0.082418</td>\n",
       "      <td>-0.181756</td>\n",
       "      <td>-0.019078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15650</th>\n",
       "      <td>music</td>\n",
       "      <td>-0.201312</td>\n",
       "      <td>0.352136</td>\n",
       "      <td>-0.126266</td>\n",
       "      <td>-0.192007</td>\n",
       "      <td>0.167450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>film</td>\n",
       "      <td>-0.249245</td>\n",
       "      <td>0.567778</td>\n",
       "      <td>-0.145280</td>\n",
       "      <td>-0.196645</td>\n",
       "      <td>0.023392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>mr</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.019606</td>\n",
       "      <td>0.289936</td>\n",
       "      <td>-0.458096</td>\n",
       "      <td>0.051266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25597 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  business  entertainment  politics     sport      tech\n",
       "25144     win -0.204785      -0.071892 -0.033466  0.399809 -0.089665\n",
       "14700   match -0.166496      -0.127954 -0.040573  0.396486 -0.061463\n",
       "22890    team -0.107919      -0.126025 -0.096541  0.361067 -0.030583\n",
       "6495      cup -0.137345      -0.090435 -0.037199  0.336211 -0.071232\n",
       "12311  injury -0.122379      -0.078572 -0.049308  0.290245 -0.039987\n",
       "...       ...       ...            ...       ...       ...       ...\n",
       "23860      uk -0.102038       0.116291  0.206630 -0.170610 -0.050274\n",
       "9518     firm  0.571839      -0.288587 -0.082418 -0.181756 -0.019078\n",
       "15650   music -0.201312       0.352136 -0.126266 -0.192007  0.167450\n",
       "9433     film -0.249245       0.567778 -0.145280 -0.196645  0.023392\n",
       "15559      mr  0.097288       0.019606  0.289936 -0.458096  0.051266\n",
       "\n",
       "[25597 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_features_weights.sort_values(by=\"sport\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22913</th>\n",
       "      <td>technology</td>\n",
       "      <td>-0.201781</td>\n",
       "      <td>-0.176289</td>\n",
       "      <td>-0.056054</td>\n",
       "      <td>-0.117837</td>\n",
       "      <td>0.551962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15320</th>\n",
       "      <td>mobile</td>\n",
       "      <td>-0.105172</td>\n",
       "      <td>-0.120836</td>\n",
       "      <td>-0.062559</td>\n",
       "      <td>-0.097270</td>\n",
       "      <td>0.385838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>233</td>\n",
       "      <td>-0.191574</td>\n",
       "      <td>-0.090254</td>\n",
       "      <td>-0.025219</td>\n",
       "      <td>-0.067974</td>\n",
       "      <td>0.375021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21441</th>\n",
       "      <td>software</td>\n",
       "      <td>-0.153330</td>\n",
       "      <td>-0.084149</td>\n",
       "      <td>-0.084437</td>\n",
       "      <td>-0.044048</td>\n",
       "      <td>0.365964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16410</th>\n",
       "      <td>online</td>\n",
       "      <td>-0.059032</td>\n",
       "      <td>-0.118098</td>\n",
       "      <td>-0.084213</td>\n",
       "      <td>-0.079578</td>\n",
       "      <td>0.340920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16490</th>\n",
       "      <td>orange</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>-0.016963</td>\n",
       "      <td>-0.015476</td>\n",
       "      <td>-0.019532</td>\n",
       "      <td>-0.114828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20102</th>\n",
       "      <td>said</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>0.040237</td>\n",
       "      <td>0.150887</td>\n",
       "      <td>-0.070312</td>\n",
       "      <td>-0.117303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10599</th>\n",
       "      <td>government</td>\n",
       "      <td>0.195998</td>\n",
       "      <td>-0.244397</td>\n",
       "      <td>0.349831</td>\n",
       "      <td>-0.150857</td>\n",
       "      <td>-0.150576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22695</th>\n",
       "      <td>t</td>\n",
       "      <td>-0.107107</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.042036</td>\n",
       "      <td>0.187509</td>\n",
       "      <td>-0.159156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>s</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>0.129987</td>\n",
       "      <td>-0.049067</td>\n",
       "      <td>0.096388</td>\n",
       "      <td>-0.182781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25597 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  business  entertainment  politics     sport      tech\n",
       "22913  technology -0.201781      -0.176289 -0.056054 -0.117837  0.551962\n",
       "15320      mobile -0.105172      -0.120836 -0.062559 -0.097270  0.385838\n",
       "500           233 -0.191574      -0.090254 -0.025219 -0.067974  0.375021\n",
       "21441    software -0.153330      -0.084149 -0.084437 -0.044048  0.365964\n",
       "16410      online -0.059032      -0.118098 -0.084213 -0.079578  0.340920\n",
       "...           ...       ...            ...       ...       ...       ...\n",
       "16490      orange  0.166800      -0.016963 -0.015476 -0.019532 -0.114828\n",
       "20102        said -0.003508       0.040237  0.150887 -0.070312 -0.117303\n",
       "10599  government  0.195998      -0.244397  0.349831 -0.150857 -0.150576\n",
       "22695           t -0.107107       0.036718  0.042036  0.187509 -0.159156\n",
       "20051           s  0.005473       0.129987 -0.049067  0.096388 -0.182781\n",
       "\n",
       "[25597 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_features_weights.sort_values(by=\"tech\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Another example!\n",
    "\n",
    "Now, let's look at a different dataset - fake_job_postings.csv from [kaggle](https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction). This file contains 18K job descriptions out of which about 800 are fake. We want the classifier to learn to classify job postings based on their descriptions. Firstly, let's do some preprocessing: Select the data column we want and remove NaN entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  fraudulent\n",
       "0  Food52, a fast-growing, James Beard Award-winn...           0\n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...           0\n",
       "2  Our client, located in Houston, is actively se...           0\n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...           0\n",
       "4  JOB TITLE: Itemization Review ManagerLOCATION:...           0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fake_jobs_dataset_file = \"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/datasets/fake_job_postings.csv\"\n",
    "\n",
    "''' uncomment if you want to run it locally '''\n",
    "fake_jobs_dataset_file = \"./datasets/fake_job_postings.csv\"\n",
    "\n",
    "job_full_df = pd.read_csv(fake_jobs_dataset_file)\n",
    "job_df = job_full_df[[\"description\", \"fraudulent\"]]\n",
    "job_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, if the description is real it has a value of 0 in the \"fraudulent\" column, 1 otherwise. Let's create a separate column in the dataframe with the textual label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-6e95b83e74ea>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  job_df[\"label\"] = job_df.fraudulent.apply(lambda x: \"real\" if x == 0 else \"fake\")\n"
     ]
    }
   ],
   "source": [
    "job_df[\"label\"] = job_df.fraudulent.apply(lambda x: \"real\" if x == 0 else \"fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "      <td>0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>Just in case this is the first time you’ve vis...</td>\n",
       "      <td>0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17876</th>\n",
       "      <td>The Payroll Accountant will focus primarily on...</td>\n",
       "      <td>0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17877</th>\n",
       "      <td>Experienced Project Cost Control Staff Enginee...</td>\n",
       "      <td>0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>Nemsia Studios is looking for an experienced v...</td>\n",
       "      <td>0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17879</th>\n",
       "      <td>Who are we?Vend is an award winning web based ...</td>\n",
       "      <td>0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17880 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description  fraudulent label\n",
       "0      Food52, a fast-growing, James Beard Award-winn...           0  real\n",
       "1      Organised - Focused - Vibrant - Awesome!Do you...           0  real\n",
       "2      Our client, located in Houston, is actively se...           0  real\n",
       "3      THE COMPANY: ESRI – Environmental Systems Rese...           0  real\n",
       "4      JOB TITLE: Itemization Review ManagerLOCATION:...           0  real\n",
       "...                                                  ...         ...   ...\n",
       "17875  Just in case this is the first time you’ve vis...           0  real\n",
       "17876  The Payroll Accountant will focus primarily on...           0  real\n",
       "17877  Experienced Project Cost Control Staff Enginee...           0  real\n",
       "17878  Nemsia Studios is looking for an experienced v...           0  real\n",
       "17879  Who are we?Vend is an award winning web based ...           0  real\n",
       "\n",
       "[17880 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-4f3a79f4cf63>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  job_df.dropna(axis=0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "job_df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real    17014\n",
      "fake      865\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_df.label.value_counts().plot.bar()\n",
    "print(job_df.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this dataset is strongly imbalanced. Since the whole dataset is quite small, we will need to add the `stratify` parameter to the `train_test_split()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate countvectorizer \n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    job_df['description'],\n",
    "    job_df['label'],\n",
    "    test_size=0.15,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "X_train_trans = count_vector.fit_transform(X_train)\n",
    "\n",
    "lr = LogisticRegression(random_state=0, penalty='l2', solver='liblinear', verbose=0)\n",
    "lr.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7619047619047619"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_trans = count_vector.transform(X_test)\n",
    "predictions = lr.predict(X_test_trans)\n",
    "\n",
    "f1_score(y_test, predictions, pos_label=\"fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "76% is not a bad score without preprocessing! Let's explore the confusion table, which will give us an insight into classifier decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(lr, X_test_trans, y_test) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the classfifier more often mislabels fake postings with the \"real\" label (40), than real postings with the \"fake\" label (15). This is probably due to the high imbalance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check some true and fake job descriptions manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_true = job_df[job_df['label'] == \"real\"].iloc[0, 0]\n",
    "job_fake = job_df[job_df['label'] == \"fake\"].iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Food52, a fast-growing, James Beard Award-winning online food community and crowd-sourced and curated recipe hub, is currently interviewing full- and part-time unpaid interns to work in a small team of editors, executives, and developers in its New York City headquarters.Reproducing and/or repackaging existing Food52 content for a number of partner sites, such as Huffington Post, Yahoo, Buzzfeed, and more in their various content management systemsResearching blogs and websites for the Provisions by Food52 Affiliate ProgramAssisting in day-to-day affiliate program support, such as screening affiliates and assisting in any affiliate inquiriesSupporting with PR &amp; Events when neededHelping with office administrative work, such as filing, mailing, and preparing for meetingsWorking with developers to document bugs and suggest improvements to the siteSupporting the marketing and executive staff'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IC&amp;E Technician | Bakersfield, CA Mt. PosoPrincipal Duties and Responsibilities:\\xa0Calibrates, tests, maintains, troubleshoots, and installs all power plant instrumentation, control systems and electrical equipment.Performs maintenance on motor control centers, motor operated valves, generators, excitation equipment and motors.Performs preventive, predictive and corrective maintenance on equipment, coordinating work with various team members.Designs and installs new equipment and/or system modifications.Troubleshoots and performs maintenance on DC backup power equipment, process controls, programmable logic controls (PLC), and emission monitoring equipment.Uses maintenance reporting system to record time and material use, problem identified and corrected, and further action required; provides complete history of maintenance on equipment.Schedule, coordinate, work with and monitor contractors on specific tasks, as required.Follows safe working practices at all times.Identifies safety hazards and recommends solutions.Follows environmental compliance work practices.Identifies environmental non-compliance problems and assist in implementing solutions.Assists other team members and works with all departments to support generating station in achieving their performance goals.Trains other team members in the areas of instrumentation, control, and electrical systems.Performs housekeeping assignments, as directed.Conduct equipment and system tagging according to company and plant rules and regulations.Perform equipment safety inspections, as required, and record results as appropriate.\\xa0Participate in small construction projects.\\xa0 Read and interpret drawings, sketches, prints, and specifications, as required.Orders parts as needed to affect maintenance and repair.Performs Operations tasks on an as-needed basis and other tasks as assigned.Available within a reasonable response time for emergency call-ins and overtime, plus provide acceptable off-hour contact by phone and company pager.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Excellent Verbal and Written Communications Skills:Ability to coordinate work activities with other team members on technical subjects across job families.Ability to work weekends, holidays, and rotating shifts, as required.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fake': 0.003077564583769843, 'real': 0.9969224354162302}\n",
      "{'fake': 0.9887470101434559, 'real': 0.011252989856544137}\n"
     ]
    }
   ],
   "source": [
    "print(dict(zip(lr.classes_, *lr.predict_proba(count_vector.transform([job_true])))))\n",
    "print(dict(zip(lr.classes_, *lr.predict_proba(count_vector.transform([job_fake])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So which words are crucial for our classifier (have highest positive & negative weights)? This is a binomial LR, so there is only one dimension of weights (negative will be mapped to 0 - true job posting, positives will be mapped to 1 - fake job posting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_feature_weights = pd.DataFrame({\"feature_name\" : count_vector.get_feature_names(), \"feature_weight\" : lr.coef_[0]})\n",
    "job_feature_weights.sort_values(by=\"feature_weight\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41910</th>\n",
       "      <td>rohan</td>\n",
       "      <td>-1.939682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26949</th>\n",
       "      <td>link</td>\n",
       "      <td>-1.529308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1000</td>\n",
       "      <td>-1.142240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30055</th>\n",
       "      <td>money</td>\n",
       "      <td>-1.105888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44224</th>\n",
       "      <td>sites</td>\n",
       "      <td>-1.094543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21884</th>\n",
       "      <td>having</td>\n",
       "      <td>0.879387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27071</th>\n",
       "      <td>ll</td>\n",
       "      <td>0.939649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45287</th>\n",
       "      <td>specialist</td>\n",
       "      <td>0.952946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52356</th>\n",
       "      <td>user</td>\n",
       "      <td>0.959319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11165</th>\n",
       "      <td>corp</td>\n",
       "      <td>1.017384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56530 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_name  feature_weight\n",
       "41910        rohan       -1.939682\n",
       "26949         link       -1.529308\n",
       "87            1000       -1.142240\n",
       "30055        money       -1.105888\n",
       "44224        sites       -1.094543\n",
       "...            ...             ...\n",
       "21884       having        0.879387\n",
       "27071           ll        0.939649\n",
       "45287   specialist        0.952946\n",
       "52356         user        0.959319\n",
       "11165         corp        1.017384\n",
       "\n",
       "[56530 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top words making the posting true and top words making the posting false.\n",
    "job_feature_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the word \"money\" has been registered by the classifier to have a very strong influence (high negative coefficient) on the \"fakeness\" of the job posting. Based on this, one can conclude that job descriptions containing many words \"money\" are more likely to be fake - perhaps their authors try to convince potential employees by an unnatural financial approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feel free to experiment with both datasets, different classifiers and their parameters!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3afa0fa510e8fcdc377ad5072d6b62de959f50d8f4193a0bfb045b2c849d2b0a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
