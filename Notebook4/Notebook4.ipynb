{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 4 - Naive-Bayes and Logistics Regression in NLP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Naive-Bayes Classifier\n",
    "\n",
    "As stated in the previous notebook, Naive-Bayes is a supervised learning probabilistic classifier. It is based on applying Bayes' probability theorem and using the fact that the occurrence of an event impacts the probability of another event. But how exactly does it work?\n",
    "\n",
    "### 7.1. NB Fundamentals\n",
    "The general purpose of classifiers is to *classify* samples from the dataset into 2 or more **classes**. Since we want to classify text, instead of the term *sample* we will use the term **document**. Thus, classifiers' task is to take an input document *d* and out of all possible classes, return a class *c*, to which the document *d* belongs.\n",
    "\n",
    "Now, since NB is the probabilistic classifier, its role would be to **maximize the probability** of the predicted class c given the input document d.\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq1.png\" alt=\"!!!equation 4.1!!!\" width=\"300\"/></div>\n",
    "\n",
    "The intuition of Bayesian classification is to use **Bayes’ rule** to transform the equation above into their probabilities that have some useful properties.\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq2.png\" alt=\"!!!equation 4.2!!!\" width=\"300\"/></div>\n",
    "\n",
    "We then substitute the first equation into the second one to get:\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq3.png\" alt=\"!!!equation 4.3!!!\" width=\"400\"/></div>\n",
    "\n",
    "We can conveniently simplify the above equation by dropping the denominator *P(d)*. This is possible because we will be computing *P(d|c)P(c) / P(d)* for each possible class, but *P(d)* does not change for each class; we are always asking the most likely class for the same article *d*, which must have the same probability *P(d)*. Thus, we can choose the class that maximizes the simpler formula\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq4.png\" alt=\"!!!equation 4.4!!!\" width=\"400\"/></div>\n",
    "\n",
    "Okay, but how do we represent a document *d*? We can represent a document as a set of **features** `d = (f1, f2, f3 ... fn)`. One way to define these features is to use the Bag-of-words model introduced in Notebook 2. After constructing the BOW of the complete dataset, we will be able to express each document as a vector of word counts. Thus, we can treat each vector value associated with a different word as a separate feature giving us information on the words (and optionally their counts) used in the document. Here we also introduce the first of two **simplifying assumptions**: since we use BOW, the **word order doesn't matter**. We don't care about the position of a word in a document.\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq5.png\" alt=\"!!!equation 4.6!!!\" width=\"400\"/></div>\n",
    "\n",
    "However, calculating `P(f1, f2, f3 ... fn | c)` requires computing all possible combinations of features (if BOW uses sum pooling than even more!). We need another simplifying assumption called the **naive Bayes assumption** - the conditional independence between features given the same class. Hence, we can multiply probabilites as follows:\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq6.png\" alt=\"!!!equation 4.7!!!\" width=\"400\"/></div>\n",
    "\n",
    "Resulting in a final equation:\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq8.png\" alt=\"!!!equation 4.8!!!\" width=\"400\"/></div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2 NB Training\n",
    "\n",
    "So how do we train the classifier? How does it learn what is *P(c)* and *P(f|c)*? Starting with the first probability, we can simply use frequencies and derive it from the probability definition: the probability of a class in the dataset is the number of documents of this class divided by the total number of all documents. \n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq8,5.png\" alt=\"!!!equation 4.11!!!\" width=\"200\"/></div>\n",
    "\n",
    "Learning the probability of features given a class *P(f<sub>i</sub>|c)* isn't more complicated. We assume a feature is just the existence of a word in the document’s bag of words (set of the vocabulary *V*), and so we’ll want *P(w|c)*, which we compute as **fraction of times the word w<sub>i</sub> appears among all words in all documents of topic c**.\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq9.png\" alt=\"!!!equation 4.12!!!\" width=\"400\"/></div>\n",
    "\n",
    "Let's consider the following example:\n",
    "\n",
    "This is our training data:\n",
    "\n",
    "| **Text** | **Labels** |\n",
    "|----------|-----------|\n",
    "|\"What a great match\" | sports |\n",
    "|\"The election results will be out tomorrow\"| not sports |\n",
    "|\"The match was very boring\"| sports |\n",
    "|\"It was a close election\"| not sports |\n",
    "\n",
    "To make the example easier to follow, let’s assume we applied some pre-processing to the sentences and removed stopwords. The resulting sentences are:\n",
    "\n",
    "| **Text** | **Labels** |\n",
    "|----------|------------|\n",
    "|\"great match\"|  sports |\n",
    "|\"election results tomorrow\"| not sports |\n",
    "|\"match boring\"| sports |\n",
    "|\"close election\"| not sports |\n",
    "\n",
    "In our small corpus, we have 2 classes each having 2 senteces. Hence, the probability of each class is:\n",
    "\n",
    "        P(\"sports\") = 2/4 = 0.5\n",
    "        P(\"not sports\") = 2/4 = 0.5\n",
    "\n",
    "Total unique features (words) for \"sports\": 3\n",
    "Total unique features (words) for \"not sports\": 4\n",
    "\n",
    "Let's say we want to assign a class to the following sentence \"that was a very close, great match\". After stop word removal it is \"**close great match**\". Now we need to perform some calculations:\n",
    "\n",
    "1. Likelihood P(\"close great match\"|sports) = P(\"close\"|sports) * P(\"great\"|sports) * P(\"match\"|sports)\n",
    "2. Likelihood P(\"close great match\"|not sports) = P(\"close\"|not sports) * P(\"great\"|not sports) * P(\"match\"|not sports)\n",
    "\n",
    "| word | P(word\\|sports) | P(word\\|not sports)|\n",
    "|------|----------------|-------------------|\n",
    "| close | 0/3 | 1/4 |\n",
    "| great | 1/3| 0/4|\n",
    "| match | 2/3| 0/4|\n",
    "\n",
    "We suspect that the correct class is \"sport\", right? Let's see what happens with the likelihood:\n",
    "P(\"close great match\"|sports) = 0/3 * 1/3 * 2/3\n",
    "P(\"close great match\"|sports) = 0\n",
    "\n",
    "Oops... This example shows a very common situation - there were no training documents classified as \"sports\" containing the word \"close\". As a result, P(\"close\"|\"sports\") results in a painful zero, which also makes the product of probabilities equals 0. We can solve this issue using **smoothing**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.4 NB Smoothing & Unknown words\n",
    "\n",
    "\n",
    "#### Laplace smoothing\n",
    "\n",
    "Smoothing is used to avoid a situation that the classifier assigns zero probability to the whole document (as we can see above). This happens when a classifier sees a word, which IS present in the vocabulary (perhaps in a document of a different class), but it wasn't used in the given context. The intuition behind the smoothing is that we don't want the classifier to assign zero probabilities to previously unseen events - the fact that something wasn't present in the training data, doesn't guarantee that it is impossible.\n",
    "\n",
    "There are many smoothing algorithms but the simplest one is called **Laplace smoothing** or **Add-one smoothing**. The part of the classification algorithm which causes a problem is the probability of a feature given a class *P(f<sub>i</sub>|c)*, which we interpret as the **fraction of times the word w<sub>i</sub> appears among all words in all documents of topic c**. If the numerator is equal to 0, the whole probability is also equal to 0. Laplace smoothing adds 1 to each count resulting in a new formula for the probability. Note that since we artificially increment the number of occurrences of each word in the numerator and denominator, we add the size of the vocabulary |V| to the denominator. This results in the equation:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq10.png\" alt=\"!!!equation 4.14!!!\" width=\"600\"/></div>\n",
    "\n",
    "#### Add-k smoothing\n",
    "\n",
    "Add-one smoothing is not the only solution, and very often not the best one. Remember, that the probability mass is finite, which means that if we add some probability to one event, we have to remove it (preferably uniformly) from other events. Adding 1 to each word count, sometimes results in moving too much probability mass from rarely seen to totally unseen events. How can we deal with this situation? Well, the simplest solution is to add a smaller number than 1 to each word count (we don't associate this value with the word count anymore). \n",
    "\n",
    "This smoothing is called **Add-k smoothing** since it adds **k** to each word count. This, however, requires estimating what is the best value for **k**. It can be very different for different datasets and applications, so it should be adjusted to the problem.\n",
    "\n",
    "#### Unseen words\n",
    "\n",
    "How about previously unseen words in none of the contexts (classes)? For example what happens if we try to test the classifier above on a document \"*close funny match*\"? Well, since we use the bag-of-words model, we have already seen (in Notebook 2) that this issue is unsolvable in a simple way because we would have to modify all one-hot vectors. The only reasonable solution, in this case, is to **remove all previously unseen words**. Because of this, NB classifiers need rather big training datasets to perform well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.5 Completed NB example & Implementation\n",
    "\n",
    "Ok, so let's complete our example using the Laplace smoothing.\n",
    "\n",
    "The size of our vocabulary |V| = 7, so we will add it to all denominators.\n",
    "\n",
    "| word | P(word\\|sports) | P(word\\|not sports)|\n",
    "|------|----------------|-------------------|\n",
    "| close | 1/10 | 2/11 |\n",
    "| great | 2/10| 1/11|\n",
    "| match | 3/10| 1/11|\n",
    "\n",
    "P(\"close great match\"|sports) = 1/10 * 2/10 * 3/10 = 0.006\n",
    "P(\"close great match\"|not sports) = 2/11 * 1/11 * 1/11 = 0.0015\n",
    "\n",
    "Now, we have to multiply each probability by the probability of a class (posterior = likelihood * prior), which results in:\n",
    "P(\"close great match\"|sports)*P(sports) = 0.006 * 0.5 = 0.003\n",
    "P(\"close great match\"|not sports)*P(not sports) = 0.0015 * 0.5 = 0.00075\n",
    "\n",
    "Thus, there is a higher probability that the test document is indeed about sports and this would be the decision of the NB classifier.\n",
    "\n",
    "Now, let's implement the same example using `Python` and `scikit-learn`! Firstly let's create a training set and a test document."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1 - sports, 0 - not sports\n",
    "training_corpus = [\"What a great match\",\n",
    "          \"The election results will be out tomorrow\",\n",
    "          \"The match was very boring\",\n",
    "          \"It was a close election\",\n",
    "          ]\n",
    "training_labels = [1, 0, 1, 0]\n",
    "\n",
    "test_doc = \"that was a very close, great match\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need to preprocess and vectorize documents to extract features (words)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate countvectorizer. You can sepcify what kind of preprocessing will be done by the CountVectorizer\n",
    "# In this case we want all text in lowercase, remove stopwords, keep only alphanumeric characters.\n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "\n",
    "# Fit training data\n",
    "training_data = count_vector.fit_transform(training_corpus)\n",
    "# Let's inspect vectors\n",
    "print(count_vector.get_feature_names())\n",
    "print(training_data.toarray())\n",
    "\n",
    "# transform test data\n",
    "test_doc_transform = count_vector.transform([test_doc])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After vectorizing, let's create and train the NB classifier. For numerical data, we have used a Gaussian NB classifier. For the NLP applications, we will use the Multinomial version of this classifier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create the classifier object and fit data. The classifier object is by default created with the Laplace smoothing\n",
    "naive_bayes = MultinomialNB(alpha=1)  # alpha parameter specifies the k number from the add-k smoothing. 1 is the default value\n",
    "naive_bayes.fit(training_data, training_labels)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = naive_bayes.predict(test_doc_transform)\n",
    "predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result of 1, means the classifier predicts that the test sentence was from the \"sport\" category. Now, let's try with this one:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_doc2 = \"that was a very close, great election\"\n",
    "test_doc2_transform = count_vector.transform([test_doc2])\n",
    "predictions = naive_bayes.predict(test_doc2_transform)\n",
    "predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We can also inspect calculated probabilites.\n",
    "print(naive_bayes.classes_)  # print the order of classes\n",
    "print(naive_bayes.predict_proba(test_doc_transform))\n",
    "print(naive_bayes.predict_proba(test_doc2_transform))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This was just a simple example to familiarize with the NB classifier. Let's look at a real word scenario!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.6 News classification using NB\n",
    "\n",
    "We’ll use a public dataset from the BBC comprised of 2225 articles, each labeled under one of 5 categories: business, entertainment, politics, sport, or tech. Articles have been already made lowercase and cleaned from punctuation.\n",
    "The goal is to train the classifier to predict what is the class of a given article. Let's start with loading the dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "bbc_data = pd.read_csv(\"datasets/bbc-text.csv\")\n",
    "bbc_data = bbc_data[['category', 'text']]\n",
    "bbc_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To develop a well-working classifier it is important to know the structure of the dataset. Let's see what is the size of each class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bbc_data['category'].value_counts().plot.bar()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, the dataset seems to be balanced. With this sort of data, imbalance in the dataset wouldn't be that problematic, but read this article if you are interested in when imbalanced data can become a problem: [https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5](https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5)\n",
    "\n",
    "Now, let's vectorize documents and train the model!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedShuffleSplit,\n",
    "    cross_val_score)\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    bbc_data['text'],\n",
    "    bbc_data['category'],\n",
    "    test_size=0.2,\n",
    "    random_state=50,\n",
    ")\n",
    "\n",
    "#instantiate countvectorizer \n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "\n",
    "#fit training data\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "#transform test data\n",
    "testing_data = count_vector.transform(X_test)\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data,y_train)\n",
    "\n",
    "predictions = naive_bayes.predict(testing_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print('f1: {}'.format(f1_score(y_test, predictions, average='macro')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wow - that’s pretty good. But maybe we were lucky and got an \"easy\" 20% test set. Let's use cross-validation to exclude that possibility"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_whole = count_vector.fit_transform(X_train)\n",
    "# CV in sklearn needs lables in numerical values, so let's use LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y_train)\n",
    "scores = cross_val_score(naive_bayes, X_whole, y_enc, cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=100), scoring='accuracy')  # other scoring metric: f1_macro, f1_micro\n",
    "# Mean accuracy scoring \n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This looks like a pretty decent score! Let's manually see how it works:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tech_article = X_train[0]\n",
    "business_article = X_train[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tech_article[:100]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "business_article[:100]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see what is the predicted class for the `tech_article`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(naive_bayes.predict(count_vector.transform([tech_article])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# And let's inspect probabilities assigned to all of the classes\n",
    "dict(zip(naive_bayes.classes_, *naive_bayes.predict_proba(count_vector.transform([tech_article]))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "How about the `business_article`?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(naive_bayes.predict(count_vector.transform([business_article])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# And let's inspect probabilities assigned to all of the classes\n",
    "dict(zip(naive_bayes.classes_, *naive_bayes.predict_proba(count_vector.transform([business_article]))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Logistic Regression Classifier\n",
    "\n",
    "### 8.1. LR Fundamentals\n",
    "\n",
    "\n",
    "Logistic Regression is the another supervised learning algorithm and probabilistic classifier. This means that it will be also interested in calculating the probability *P(c|d)*:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq1.png\" alt=\"!!!equation 4.1!!!\" width=\"300\"/></div>\n",
    "\n",
    "NB classifier used the Bayes' theorem and additional assumptions to derive another equation from the one above. NB uses the probability of a document as a set of features given a class *P(d|c)*, which was learned from the training dataset. \n",
    "\n",
    "Logistic regression does it differently. As a discriminative classifier, it tries to learn differences between classes rather than how class representatives \"look like\". Thus, it will directly try to learn the probability of a class given a set of features representing a document *P(c|d)*. The key of this algorithm is in determining which features discriminate documents most efficiently, by assigning these features appropriate **weights** (parameters). For example, in the \"sports\" or \"not sports\" text classification, the word \"football\" will probably\n",
    "have strong positive weight and the word \"princess\", probably strong negative weight. \n",
    "\n",
    "In supervised machine learning, we have input features and sets of labels. To make predictions based on data, we use a function F with some parameters Θ to map features to output labels. To get an optimum mapping from features to labels, we have to minimize the cost function, which works by comparing how closely the output Ŷ is to the true labels Y from the training data. Learning takes place by updating the parameters and repeating the process until the cost is minimized.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/sup_learning.jpeg\" alt=\"!!!equation 4.1!!!\" width=\"600\"/></div>\n",
    "\n",
    "Parts of the Logistic Regression algorithm:\n",
    "1. Feature representation - in our case using one-hot vectors, but in general, using **word embeddings** (Notebook 5)\n",
    "2. A classification function that gives the estimated class using *P(c|d)* - we will use two most popular ones - **sigmoid** (binomial LR) and **softmax** (multinomial LR)\n",
    "3. The cost function (or loss function) used for learning, which tells us the difference between the estimated class ĉ and the true class c. We will use **cross-entropy loss function**\n",
    "4. An algorithm for optimizing the classifier by adjusting parameters and consequently minimizing the cost function. One of them is the **Stochastic Gradient Decent**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we represent features using a vector `x=[x`<sub>`1`</sub>`, x`<sub>`2`</sub>`, x`<sub>`3`</sub>` ... x`<sub>`n`</sub>`]` and we need to associate some weights with each of these features, we can also represent these weights using a **weights vector** `w=[w`<sub>`1`</sub>`, w`<sub>`2`</sub>`, w`<sub>`3`</sub>` ... w`<sub>`n`</sub>`]`. Then, we can calculate the score for the test document by multiplying each feature with the associated weight and adding all results together. There is also another parameter - the **bias term** `b`, which is added to the sum.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq11.png\" alt=\"!!!equation 5.2!!!\" width=\"300\"/></div>\n",
    "\n",
    "In other words, we want the sum of the **dot product** of the features and weights vectors and the bias term:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq12.png\" alt=\"!!!equation 5.3!!!\" width=\"300\"/></div>\n",
    "\n",
    "But how do we map this score to the probability, since the score can be any real number from −∞ to ∞? We will use a **sigmoid function** *σ(z)*, which is also called the logistic function.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq13.png\" alt=\"!!!equation 5.3!!!\" width=\"700\"/></div>\n",
    "\n",
    "As you can see, it is symmetric with respect to the point (0, 0.5) and it maps all real numbers to the range (0,1) - this is exactly what we need! For two classes (binomial logistic regression), class 0 and class 1, we have probabilities as follows:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"nb_resources/eq14.png\" alt=\"!!!equation 5.3!!!\" width=\"400\"/></div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss function\n",
    "\n",
    "Okay, so how does the classifier learn the correct weights vector and the bias? It starts with some arbitrary values, which are then used to classify a training set. Each result is compared with the true label using the loss functions, in our case with the cross-entropy loss function. This function tells the classifier what is the difference between the classifier output and the true label. Taking into account loss, the classifier needs to adjust its parameters *θ* (in our case *θ = w, b*) to minimize the loss. This is equivalent to finding parameters *θ*, that minimize the loss function (find its minimum). \n",
    "\n",
    "#### Minimizing the loss\n",
    "\n",
    "Since the loss function is parameterized by weights and the bias term, finding its minimum is a multidimensional task. The popular algorithm for finding the minimum of a function is the Stochastic Gradient Decent (SGD). Gradient Descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters *θ*) the function’s slope is rising the most steeply, and moving in the opposite direction. The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself 360 degrees, find the direction where the ground is sloping the steepest, and walk downhill in that direction.\n",
    "\n",
    "I strongly encourage you to read [this chapter](https://web.stanford.edu/~jurafsky/slp3/5.pdf) of the book “Speech and Language Processing” by Daniel Jurafsky and James H. Martin as it explains the idea behind SGD and Logistic Regression very well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Regularization\n",
    "\n",
    "There is still at least one more thing to consider. What happens if we choose weights that make the model perfectly match the training data? This may result in overfitting - the same model may have much worse performance on the testing data. To avoid it we can add a special **regularization term**, which penalizes large weights. The reason for penalizing large weights is that they cause some features (present only in the training set) to be much more important than others. When a model is exposed to unseen data, this may block out other crucial features and their combinations.\n",
    "\n",
    "Having all this knowledge, let's try to implement a Logistic Regression classifier on the BBC dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.2. LR example using the bbc news dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bbc_data = pd.read_csv(\"datasets/bbc-text.csv\")\n",
    "bbc_data = bbc_data[['category', 'text']]\n",
    "bbc_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's vectorize words, split dataset into training and testing and fit the classifier!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#instantiate countvectorizer \n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    bbc_data['text'],\n",
    "    bbc_data['category'],\n",
    "    test_size=0.2,\n",
    "    random_state=50,\n",
    ")\n",
    "\n",
    "# remember to fit count vectorizer on the training data, because only these words are present in the training set. \n",
    "# Adding here words from the testing set makes this set no longer \"unseen\" to the model\n",
    "X_train_trans = count_vector.fit_transform(X_train)\n",
    "y_train_trans = le.fit_transform(y_train)\n",
    "\n",
    "# Logistic Regression classifier has several parameters including the penalty for big weights (l1, l2) and the method of finding the loss function minimum (solver)\n",
    "lr = LogisticRegression(random_state=0, penalty='l2', solver='lbfgs', verbose=0)\n",
    "lr.fit(X_train_trans, y_train_trans)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test_trans = count_vector.transform(X_test)\n",
    "y_test_trans = le.transform(y_test)\n",
    "\n",
    "predictions = lr.predict(X_test_trans)\n",
    "accuracy_score(y_test_trans, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, let's use CV to exclude \"lucky\" test set. It may take more time than usual."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CV in sklearn needs lables in numerical values, so let's use LabelEncoder\n",
    "\n",
    "scores = cross_val_score(lr, X_train_trans, y_enc, cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0), scoring='accuracy')  # other scoring metric: f1_macro, f1_micro\n",
    "# Mean accuracy scoring \n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see a simple example of how the classifier performs on a tech article."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tech_article = X_train[0]\n",
    "tech_article[:100]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dict(zip(le.inverse_transform(lr.classes_), *lr.predict_proba(count_vector.transform([tech_article]))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One interesting thing we can do is to inspect which words have the most positive and most negative weights (are influencing the classification most). The `lr.coef_` gives us weights trained by the classifier for each class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(lr.coef_)\n",
    "print(\"Shape: {}\".format(np.shape(lr.coef_)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, each class has separate weights (there are 5 classes and 5 dimensions), as this is a Multinomial Logistic Regression classification task. Let's see the top-weighted words for each class. Firstly, we need to associate classes names with values, then we can see which words discriminate classes in the best way."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Classes names are mapped to numbers (we used the label encoder for this, so let's see what is the order)\n",
    "class_names = le.inverse_transform(lr.classes_)\n",
    "print(class_names)\n",
    "print(lr.classes_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weights_dict = {}\n",
    "weights_dict[\"feature\"] = count_vector.get_feature_names()\n",
    "for class_num in range(5):\n",
    "    weights_dict[class_names[class_num]] = lr.coef_[class_num]\n",
    "\n",
    "bbc_features_weights = pd.DataFrame(weights_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bbc_features_weights.sort_values(by=\"business\", ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bbc_features_weights.sort_values(by=\"entertainment\", ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bbc_features_weights.sort_values(by=\"politics\", ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bbc_features_weights.sort_values(by=\"sport\", ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bbc_features_weights.sort_values(by=\"tech\", ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.3 Another example!\n",
    "\n",
    "Now, let's look at a different dataset - fake_job_postings.csv. This file contains 18K job descriptions out of which about 800 are fake. We want the classifier to learn to classify job postings based on their descriptions. Firstly, let's do some preprocessing: Select the data column we want and remove NaN entries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "job_full_df = pd.read_csv(\"datasets/fake_job_postings.csv\")\n",
    "job_df = job_full_df[[\"description\", \"fraudulent\"]]\n",
    "job_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "job_df.dropna(axis=0, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "job_df['fraudulent'].value_counts().plot.bar()\n",
    "print(job_df['fraudulent'].value_counts())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, this dataset is strongly imbalanced. Since the whole dataset is quite small, we will need to add the `stratify` parameter to the `train_test_split()` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#instantiate countvectorizer \n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    job_df['description'],\n",
    "    job_df['fraudulent'],\n",
    "    # stratify = job_df['fraudulent'],\n",
    "    test_size=0.15,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "X_train_trans = count_vector.fit_transform(X_train)\n",
    "\n",
    "lr = LogisticRegression(random_state=0, penalty='l2', solver='liblinear', verbose=0)\n",
    "lr.fit(X_train_trans, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test_trans = count_vector.transform(X_test)\n",
    "\n",
    "predictions = lr.predict(X_test_trans)\n",
    "f1_score(y_test, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, let's use CV to confirm these results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(lr, X_train_trans, y_train, cv=StratifiedShuffleSplit(n_splits=10, test_size=0.15, random_state=0), scoring='f1') \n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check some true and fake job descriptions manually:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "job_true = job_df[job_df['fraudulent'] == 0].iloc[0, 0]\n",
    "job_fake = job_df[job_df['fraudulent'] == 1].iloc[0, 0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "job_true"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "job_fake"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(dict(zip(lr.classes_, *lr.predict_proba(count_vector.transform([job_true])))))\n",
    "print(dict(zip(lr.classes_, *lr.predict_proba(count_vector.transform([job_fake])))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So which words are crucial for our classifier (have highest positive & negative weights)? This is a binomial LR, so there is only one dimension of weights (negative will be mapped to 0 - true job posting, positives will be mapped to 1 - fake job posting)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "job_feature_weights = pd.DataFrame({\"feature_name\" : count_vector.get_feature_names(), \"feature_weight\" : lr.coef_[0]})\n",
    "job_feature_weights.sort_values(by=\"feature_weight\", inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Top words making the posting true and top words making the posting false.\n",
    "job_feature_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interestingly, the word \"money\" has been registered by the classifier to have a very strong influence on the \"fakeness\" of the job posting. Based on this, one can conclude that job descriptions containing many words \"money\" are more likely to be fake - perhaps their authors try to convince potential employees by an unnatural financial approach."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feel free to experiment with both datasets, different classifiers and their parameters!"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}