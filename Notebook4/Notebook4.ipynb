{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 4 - Naive-Bayes and Logistics Regression in NLP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Naive-Bayes Classifier\n",
    "\n",
    "As stated in the previous notebook, Naive-Bayes is a supervised learning probabilistic classifier. It is based on applying Bayes' probability theorem and using the fact that occurrence of an event impacts the probability of another event. But how exactly does it work?\n",
    "\n",
    "### 7.1. NB Fundamentals\n",
    "The general purpose of classifiers is to *classify* samples from the dataset into 2 or more **classes**. Since we want to classify text, instead of term *sample* we will use term **document**. Thus, classifiers' task is to take an input document *d* and out of all possible classes, return a class *c*, to which the document *d* belongs.\n",
    "\n",
    "Now, since NB is the probabilistic classifier, its role would be to **maximize the probability** of the predicted class c given the input document d.\n",
    "<div style=\"text-align:center\"><img src=\"res/eq1.png\" alt=\"!!!equation 4.1!!!\" width=\"300\"/></div>\n",
    "\n",
    "The intuition of Bayesian classification is to use **Bayes’ rule** to transform the equation above into their probabilities that have some useful properties.\n",
    "<div style=\"text-align:center\"><img src=\"res/eq2.png\" alt=\"!!!equation 4.2!!!\" width=\"300\"/></div>\n",
    "\n",
    "We then substitute the first equation into the second one to get:\n",
    "<div style=\"text-align:center\"><img src=\"res/eq3.png\" alt=\"!!!equation 4.3!!!\" width=\"400\"/></div>\n",
    "\n",
    "We can conveniently simplify the above equation by dropping the denominator *P(d)*. This is possible because we will be computing *P(d|c)P(c) / P(d)* for each possible class, but *P(d)* does not change for each class; we are always asking the most likely class for the same article *d*, which must have the same probability *P(d)*. Thus, we can choose the class that maximises the simpler formula\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"res/eq4.png\" alt=\"!!!equation 4.4!!!\" width=\"400\"/></div>\n",
    "\n",
    "Okay, but how do we actually represent a document *d*? We can represent a document as a set of **features** `d = (f1, f2, f3 ... fn)`. One way to define these features is to use the Bag-of-words model introduced in the Notebook 2. After contstructing the BOW of the complete dataset, we will be able to express each document as a vector of word counts. Thus, we can treat each vector value associated with a different word as a separate feature giving us information on the words (and optionally their counts) used in the document. Here we also introduce the first of two **simplifying assumptions**: since we use BOW, the **word ordering doesn't matter**. We don't care about the position of a word in a document.\n",
    "<div style=\"text-align:center\"><img src=\"res/eq5.png\" alt=\"!!!equation 4.6!!!\" width=\"400\"/></div>\n",
    "\n",
    "However, calculating `P(f1, f2, f3 ... fn | c)` requires computing all possible combinations of features (if BOW uses sum pooling than even more!). We need another simplifying assumption called the **naive Bayes assumption** - the conditional independence between features given the same class. Hence, we can multiply probabilites as follows:\n",
    "<div style=\"text-align:center\"><img src=\"res/eq6.png\" alt=\"!!!equation 4.7!!!\" width=\"400\"/></div>\n",
    "\n",
    "Resulting in a final equation:\n",
    "<div style=\"text-align:center\"><img src=\"res/eq8.png\" alt=\"!!!equation 4.8!!!\" width=\"400\"/></div>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2 NB Training\n",
    "\n",
    "So how do we train the classifier? How does it learn what actually is *P(c)* and *P(f|c)*? Starting with the first probability, we can simply use frequencies and derive it from the probability definition: the probability of a class in the dataset is the number of documents of this class divided be the total number of all documents. \n",
    "<div style=\"text-align:center\"><img src=\"res/eq8,5.png\" alt=\"!!!equation 4.11!!!\" width=\"200\"/></div>\n",
    "\n",
    "Learning the probability of a features given a class *P(f<sub>i</sub>|c)* isn't more complicated. We assume a feature is just the existence of a word in the document’s bag of words (set of the vocabulary *V*), and so we’ll want *P(w|c)*, which we compute as **fraction of times the word w<sub>i</sub> appears among all words in all documents of topic c**.\n",
    "<div style=\"text-align:center\"><img src=\"res/eq9.png\" alt=\"!!!equation 4.12!!!\" width=\"400\"/></div>\n",
    "\n",
    "Let's consider the following example:\n",
    "\n",
    "This is our training data:\n",
    "\n",
    "| **Text** | **Lables** |\n",
    "|----------|-----------|\n",
    "|\"What a great match\" | sports |\n",
    "|\"The election results will be out tomorrow\"| not sports |\n",
    "|\"The match was very boring\"| sports |\n",
    "|\"It was a close election\"| not sports |\n",
    "\n",
    "To make the example easier to follow, let’s assume we applied some pre-processing to the sentences and removed stopwords. The resulting sentences are:\n",
    "\n",
    "| **Text** | **Lables** |\n",
    "|----------|------------|\n",
    "|\"great match\"|  sports |\n",
    "|\"election results tomorrow\"| not sports |\n",
    "|\"match boring\"| sports |\n",
    "|\"close election\"| not sports |\n",
    "\n",
    "In our small corpus, we have 2 classes each having 2 senteces. Hence, the probability of each class is:\n",
    "\n",
    "        P(\"sports\") = 2/4 = 0.5\n",
    "        P(\"not sports\") = 2/4 = 0.5\n",
    "\n",
    "Total unique features (words) for \"sports\": 3\n",
    "Total unique features (words) for \"not sports\": 4\n",
    "\n",
    "Let's say we want to assign a class to the following sentence \"that was a very close, great match\". After stop word removal it is \"**close great match**\". Now we need to perform some calculations:\n",
    "\n",
    "1. Likelihood P(\"close great match\"|sports) = P(\"close\"|sports) * P(\"great\"|sports) * P(\"match\"|sports)\n",
    "2. Likelihood P(\"close great match\"|not sports) = P(\"close\"|not sports) * P(\"great\"|not sports) * P(\"match\"|not sports)\n",
    "\n",
    "| word | P(word\\|sports) | P(word\\|not sports)|\n",
    "|------|----------------|-------------------|\n",
    "| close | 0/3 | 1/4 |\n",
    "| great | 1/3| 0/4|\n",
    "| match | 2/3| 0/4|\n",
    "\n",
    "We suspect that the correct class is \"sport\", right? Let's see what happens with the likelihood:\n",
    "P(\"close great match\"|sports) = 0/3 * 1/3 * 2/3\n",
    "P(\"close great match\"|sports) = 0\n",
    "\n",
    "Oops... This example shows a very common situation - there were no training documents classified as \"sports\" containing word \"close\". As a result, P(\"close\"|\"sports\") results in a painful zero, which also makes the product of probabilities equals 0. We can solve this issue using **smoothing**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.4 NB Smoothing & Unknown words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.5 Completed NB example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.6 News classification using NB"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Logistic Regression Classifier"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}