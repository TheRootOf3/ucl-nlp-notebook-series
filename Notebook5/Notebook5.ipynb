{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 5 - Feature engineering\n",
    "\n",
    "# I HAVE NO IDEA IF IT EVEN MAKES SENSE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- ## 9. Features in textual data -->\n",
    "\n",
    "Text is only a method of transcribing our thoughts - we use it to communicate with other people... and computers! However, if we give a computer raw textual data it has no idea how to interpret this set of characters (actually sequence of bits). Text written in English makes sense to us because we know how to interpret it: we look for features we were taught are meaningful (separate words, multiple words, idioms), recall in our mind what is their meaning and join them together to get the full author's notion. \n",
    "\n",
    "If we want a computer to \"understand\" the text, we first need to tell it the set of features it should look for. The process of developing these features is called **feature engineering**. So what can be a feature? For different applications, there will be different features. Let's say we want the computer to classify positive and negative reviews. We may create a feature that counts all occurrences of the word *“good\"*. The higher the count, the more \"positive\" the review is. Also, we could add a feature that expresses the review’s length (e.g. longer review indicates a positive sentiment). However, the word *\"good\"* is not the only word indicating positive sentiment. We will also need to manually add word-features (\"nice\", \"cool\" etc.). This seems like a bad idea because we will be ignoring all other words present in the review.\n",
    "\n",
    "Instead, we can try to express the **meaning of each word** in a document. Then, we can represent a document as a set of meaningful words.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Vectorizers\n",
    "\n",
    "The standard representation of a word meaning in NLP is by using a vector. We can also represent whole documents using vectors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.1. Classical one-hot vectors\n",
    "\n",
    "We have already seen them in previous notebooks. One-hot vector can be used to represent a document from the corpus based on words the document contains.\n",
    "Let's say we have a corpus C of 3 documents d<sub>1</sub>, d<sub>2</sub>, d<sub>3</sub>.\n",
    "\n",
    "| document number | document content |\n",
    "|-----------------|------------------|\n",
    "| d<sub>1</sub>   | \"i like apples\"  |\n",
    "| d<sub>2</sub>   | \"we like dogs\"   |\n",
    "| d<sub>3</sub>   | \"we and i and dogs\" |\n",
    "\n",
    "Now, we can associate each word with a term in a vector: [\"i\", \"like\", \"apples\", \"we\", \"dogs\", \"and\"], and represent documents using vectors (using max pooling).\n",
    "\n",
    "| document number | document content |\n",
    "|-----------------|------------------|\n",
    "| d<sub>1</sub>   | [1, 1, 1, 0, 0, 0]  |\n",
    "| d<sub>2</sub>   | [0, 1, 0, 1, 1, 0]  |\n",
    "| d<sub>3</sub>   | [1, 0, 0, 1, 1, 1]  |\n",
    "\n",
    "Ok, done! But how can we learn the meanings of these words? Let's build the term-document matrix!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.2. Term-document matrix\n",
    "\n",
    "The term-document matrix is the representation of word occurrences in documents from the corpus. It shows how many times each word occurs in each document. Let's visualize it using our corpus C.\n",
    "\n",
    "|          | d<sub>1</sub> | d<sub>2</sub> | d<sub>3</sub> |\n",
    "|----------|---|---|---|\n",
    "| \"i\"      | 1 | 0 | 1 |\n",
    "| \"like\"   | 1 | 1 | 0 |\n",
    "| \"apples\" | 1 | 0 | 0 |\n",
    "| \"we\"     | 0 | 1 | 1 |\n",
    "| \"dogs\"   | 0 | 1 | 1 |\n",
    "| \"and\"    | 0 | 0 | 2 |\n",
    "\n",
    "If you look at columns you can read vectors representing each document. For example the document d<sub>3</sub> is represented by vector [1,0,0,1,1,2]. \n",
    "\n",
    "Now, let's look at another, more advanced term-document matrix for a corpus of 4 Shakespeare plays and 4 selected words from them. Note what is the relation in word occurrences between comedies (As You Like It and Twelfth Night) and other plays (Julius Caesar and Henry V).\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/pic1.png\" alt=\"\" width=\"800\"/></div>\n",
    "\n",
    "#### Documents similarity\n",
    "\n",
    "We can use these counts to determine whether the two documents are similar. The intuitive approach is that **two documents are similar if they have similar vectors** - in other words, they contain similar words.\n",
    "\n",
    "Again, we can represent each document using word counts creating a 4d vector. Since it's impossible to display a 4-dimensional plot, let's see how we can compare these documents using only counts of words \"battle\" and \"fool\" using 2d vectors.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/pic2.png\" alt=\"\" width=\"600\"/></div>\n",
    "\n",
    "As you can see, these two words are discriminating documents very well! As expected, in comedies there will be a higher number of \"fools\" and barely visible \"battles\", hence based on the words \"battle\" and \"fool\", documents \"As You Like It\" and \"Twelfth Night\" are similar. The same applies to \"Henry V\" and \"Julius Caesar\", where the number of \"battles\" is higher than the number of \"fools\". Of course, in normal implementation, we would use vectors of length |V| to represent a document.\n",
    "\n",
    "\n",
    "#### Words similarity\n",
    "\n",
    "However, what is interesting, we can also read **row vectors**! We can use them to represent the **meaning of each word**.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/pic3.png\" alt=\"\" width=\"800\"/></div>\n",
    "\n",
    "For example, we can express the word \"fool\" using a vector [36, 58, 1, 4] and the word \"wit\" using another vector [20, 15, 2, 3]. Here we can apply the same intuition as to documents: **Two words are similar when they have similar vectors** - they tend to occur in similar documents. So in theory, word \"fool\" [36, 58, 1, 4] should be more similar to \"wit\" [20, 15, 2, 3] than to \"battle\" [1, 0, 7, 13] and indeed, it is true!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.3. Term-term matrix\n",
    "\n",
    "Another way to represent **word meanings** is to use the **term-term matrix**. In this matrix, both rows and columns are labeled with words from the corpus vocabulary. To construct a matrix, for each word *w* we look for the context of this word. We can define the context as *k* previous and *k* next words (usually *k* is small ~5). \n",
    "\n",
    "For example, if there is a sentence \"At the university I work on data science. My computer performance is quite low.\" and we look for a context +/- 4 words of the word \"computer\", we will get words: \"on\", \"data\", \"science\", \"My\", \"performance\", \"is\", \"quite\", \"low\". Now, every pair of the word *w* and each of these context words gets +1 in the term-term matrix. If we take every word from V look for its context, and note counts of each co-occurrence, we will develop this word-word co-occurrence matrix. \n",
    "\n",
    "The intuition behind this is that if two words co-occur (are nearby in text), their meaning is similar. Let's look at the subset of the term-term matrix for the Wikipedia corpus.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/pic4.png\" alt=\"\" width=\"800\"/></div>\n",
    "\n",
    "As you can see, the word \"digital\" co-occurs more often with \"computer\" (1670) than with \"sugar\" (4). On the other hand, \"cherry\" is much often seen together with \"pie\" (442) than with \"computer\" (2). In other words, \"cherry\" is more similar to \"pie\" than to \"computer\", which makes sense."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.4 and 1/2 Cosine similarity measure\n",
    "\n",
    "Since words and documents similarities will be defined by their vectors similarities, we need to define a **measure of vector similarity**, which takes two vectors as an input and returns a measure of their similarity. Visually the intuition for this is that two vectors are most similar when they are equal (point in the same direction). Two vectors are different if they point in exactly opposite directions. But how do we define it mathematically for multidimensional vectors? \n",
    "\n",
    "The most common similarity metric is the **cosine of the angle between two vectors**. Intuitively, if the angle between two vectors is small (cosine is large) then these two vectors point in a similar direction. If the angle between two vectors is large (cosine is small) then these vectors point in different directions.\n",
    "\n",
    "The cosine of two vectors *v* and *w* is given by the following equation:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/eq1.png\" alt=\"\" width=\"400\"/></div>\n",
    "\n",
    "where |v| means the length of the vector *v*. However, why do we divide by these lengths? Vectors differ not only in direction but also in length. Generally, since vector terms represent word counts in documents (term-document matrix) or word co-occurrences (term-term matrix), frequent words will have much longer vectors than rare words. Thus, we need to **normalize** their lengths so they don't affect the measure - we want to know how much two vectors differ regardless of their lengths. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.4. TF-IDF\n",
    "\n",
    "Both term-document and term-term matrices are based on the frequencies of words. Because of this, very frequent words like \"the\" or \"he\", do not discriminate well because they are too general. Hence, we need different methods, which will punish too frequent words and discriminate well. Let's introduce the first one, based on the term-document matrix.\n",
    "\n",
    "TF-IDF, Term Frequency - Inverse Document Frequency is of the same format as term-document matrix, but values are calculated in two steps:\n",
    "\n",
    "**Term Frequency** of a term *t* in a document *d* is simply a logarithm of the number term *t* occurs in *d*: \n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/eq2.png\" alt=\"\" width=\"300\"/></div>\n",
    "\n",
    "We use a logarithm to \"punish\" words which occur very frequently. The number 1 inside the logarithm is added since if there are no occurrences of a word, we would take a logarithm of 0, which is undefined.\n",
    "\n",
    "The next part of the TF-IDF is called **Inverse Document Frequency** and emphasizes words, that are rare (may discriminate well). Document frequency for a term *t* is simply the number of documents, in which *t* occurs. Since we are intrested in its opposite (we need rare words!), IDF is calculated as:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/eq3.png\" alt=\"\" width=\"200\"/></div>\n",
    "\n",
    "The complete TF-IDF weighted value for a term *t* and a document *d* is given by the product of both TF and IDF:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/eq4.png\" alt=\"\" width=\"200\"/></div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.5. PPMI\n",
    "\n",
    "The equivalent of the TF-IDF for a term-term matrix is the **Positive Pointwise Mutual Information**. The fundamental idea behind this method is to learn how much more two words occur together than it is expected by assuming they are independent. If they occur less often, we will not take them into account and just replace the value with 0 (hence it's called *Positive*). The formula for the PPMI of a word *w* in the context *c* is given by:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/eq5.png\" alt=\"\" width=\"400\"/></div>\n",
    "\n",
    "It has been found that when the term *α* is added (α = 0.75), PPMI performance is improved."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Word embeddings\n",
    "\n",
    "All previous examples used long and sparse vectors of length |V| or |D|. For big corpora, the number of vocabulary may be oscillating around 40,000 (Brown Corpus) or even 13,000,000 (Google N-grams). This is not only a waste of space but also such long vectors are often not discriminating well and require a very long learning process. Hence, we introduce the concept of **embeddings**, dense vectors of length up to 1,000 (however often much shorter). Let's exemplify the idea behind them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imagine a small vocabulary containing 5 words: king, queen, man, woman, and princess. The one-hot vector for the queen would look like the one below.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/embed_pic0.jpeg\" alt=\"\" width=\"400\"/></div>\n",
    "\n",
    "We could try to develop a term-document or term-term matrix but this is not how embeddings work. It's more about extracting significant attributes characterizing specific words and contexts in which they are used.\n",
    "\n",
    "That words are rich entities with many layers of connotation and meaning. Let’s hand-craft some semantic features for these 5 words. We will represent each word as having some value between 0 and 1 for 5 semantic qualities: royalty, masculinity, femininity, age, and edibility.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/embed_pic1.jpeg\" alt=\"\" width=\"600\"/></div>\n",
    "\n",
    "Given the word “king”, it has a high value for the feature “royalty” (because a king is a male member of the royal family) but a low value for femininity (because he is male) and an even lower value for edibility (because we do not normally eat kings). In the above made-up toy dataset, there are 5 semantic features, and we can plot three of these at a time as a 3D scatter plot with each feature being an axis/dimension.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\".nb_resources/embed_pic2.jpeg\" alt=\"\" width=\"300\"/></div>\n",
    "\n",
    "You do not have to create word embeddings yourself. Pre-trained word embeddings can be downloaded and used in your models. We will also look into the most popular algorithms for learning static word embeddings from a corpus."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 11.1. Static vs contextual\n",
    "\n",
    "Embeddings can be generally divided into two groups. **Static embeddings** learn a single embedding for each word, independently of the context in which that word is used. This method is simpler and faster and gives decent results. On the other hand, **Contextual embeddings** are more complicated and give better performance, since in this case each word may be represented by more than one embedding (if it occurs in many different contexts). Contextual embeddings, as a relatively new concept, take advantage of complex neural networks and are generally harder to learn and develop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 11.2. word2vec\n",
    "\n",
    "Word2vec is one of the most popular algorithms for learning static embeddings. The intuition of word2vec is that instead of calculating frequencies of words co-occurrences (how often a word *w* occurs near a context word *c*?), it trains a classifier to make a decision: Is word *w* likely to occur near a context word *c*? After training the classifier, the word *w* is represented by the vector of classifier **weights**.\n",
    "\n",
    "What is interesting, to train the classifier we don't need any additional labels because we can use word co-occurrences as labels! The overview of this algorithm is as follows:\n",
    "1. Treat the target word and a neighboring context word as positive examples.\n",
    "2. Randomly sample other words in the lexicon to get negative samples.\n",
    "3. Use logistic regression to train a classifier to distinguish those two cases.\n",
    "4. Use the learned weights as the embeddings.\n",
    "\n",
    "As you can see, we will generate a training set containing both real word co-occurrences from the source (positive samples) and we will randomly generate some artificial word co-occurrences (negative samples).\n",
    "\n",
    "However, how do we define *a context* for a target word *w*? There are two approaches:\n",
    "- CBOW - continuous bag-of-words\n",
    "- SG - skip-gram\n",
    "\n",
    "**CBOW** makes a window around the target word *w* of *k* words and treats it as a context. In this case, the word order doesn't matter. CBOW is generally faster to train than SG.\n",
    "\n",
    "**SG** also creates a window of surrounding words but in this case their order and distance from the target word *w* matter. The Skip-gram approach works better for infrequent words. The popular version of SG is **skip-gram with negative sampling** (**SGNS**), where the great majority of all training samples are negative."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}