{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 6 - Putting it all together\n",
    "\n",
    "Although in this notebook we will not go over new topics, we are going to see how we can fully handle a text sentiment classification task. We are going to use many different techniques covered in previous notebooks, that (hopefully) let us develop quite an efficient model. Additionally, we are going to create some visualizations of dataset statistics and model performance, so we can understand the problem better.\n",
    "\n",
    "We are going to use the dataset containing tweets about US airlines. The dataset is stored in the CSV file `airline_tweets.csv` and contains tweet text labeled with either *\"positive\"*, *\"neutral\"* or *\"negative\"*. Besides, it contains (potentially) helpful information like the name of the airline and the user location. Let's explore it!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import swifter\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, TfidfTransformer\n",
    "from langdetect import detect\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import en_core_web_sm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_tweets = pd.read_csv(\"./datasets/airline_tweets.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_tweets.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, so there are many columns with additional information. However, some of their fields are empty. To determine whether we want to include a certain column in the further analysis, we may want to inspect how many fields in this columns are null. Let's do it using the `.info()` method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_tweets.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, now we have an overview of the dataset and we can see that columns like `airline_sentiment_gold` or `tweet_coord` are generally empty (contain null values). How about columns like `tweet_location` or `user_timezone`? We can inspect them visually and see if \"null\" values are stacked or randomly scattered throughout the whole dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.heatmap(df_tweets.isnull(), cbar=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our study, beside \"text\" and \"airline_sentiment\" we will use the \"airline\" and \"tweet_location\" columns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df_tweets[[ \"airline_sentiment\", \"airline\", \"text\", \"tweet_location\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's train the Naive-Bayes classifier without any preprocessing or parameter tuning and let's see what results we get."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tweets.text, df_tweets.airline_sentiment, test_size=0.2, \n",
    "                                                                                random_state=10)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1),binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# X_train_counts.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# X_test_counts.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_counts, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_predicted_counts = mnb.predict(X_test_counts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None, average='weighted')             \n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like even withouth much work we can get not bad results! Let's verify them using cross-validation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(mnb, X_train_counts, y_train, cv=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, so this baseline classifier run on the totally unprocessed dataset gives us **around 75% accuracy** using cross-validation. We will come back to this number later!\n",
    "\n",
    "From now we aim to get a higher accuracy score. We are going to preprocess data, extract some features and work on the model parameters selection, so all of this work will hopefully let us get better results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further dataset exploration\n",
    "\n",
    "Let's see what is the structure of the dataset in terms of the airline and sentiment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline\",kind=\"count\", data=df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In terms of airlines, the situations doesn't look very bad except for the low number of tweets regarding Virgin America."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline_sentiment\", kind=\"count\", data=df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, when it comes to the sentiment analysis, we can see that the dataset is highly imbalanced. We have much more negative tweets than neutral or positive ones. Often the result of such imbalance is the lower accuracy. To partially solve this issue we may want to *oversample* later.\n",
    "\n",
    "We may also want to see what is the sentiment statistic for each airline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.factorplot(\"airline\", data=df, hue=\"airline_sentiment\", kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What we can read from this plot? Actually quite usefull information! We can see that the airline people tweet about has a strong influence on the tweet sentiment. For example the great majority of all tweets about United Airlines are negative. On the other hand, neutral and positive tweets regarding Delta Airlines outnumber negative samples. Hence, we will definitely want to treat the tweet-related airline as a feature. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So how can we use the name of the airline as a feature? One way is to add it at the end of the text. By doing so, we will give a classifier a \"hint\", that is related to the sentiment. Interestingly, since every tweet mentions one of 6 airlines, they already contain airlines names in the beginning! Let's make sure that it is true for all tweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_starting_word = df[\"text\"].apply(word_tokenize)\n",
    "pd.Series(x[1].lower() for x in df_starting_word).value_counts().head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the great majority of all tweets start with the airline name. Some tweets do not, but we will ingnore them since these are rare cases. In the preprocessing, we will remove \"@\" characters and make all words lowercase so we will treat \"united\" and \"United\" as the same token.\n",
    "\n",
    "Hence, we can drop the \"airline\" column, since we will not use it anymore."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del df[\"airline\"]\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSA (FANCY VISUAL)\n",
    "\n",
    "(deleted)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LOCATION EXTRACTION (FEATURE)\n",
    "\n",
    "We can also extract the location from the \"tweet_location\" column. One may think that since the column already exists, the only thing we need to do is to match it with the label column. In this case this won't work, since the \"tweet_location\" column contains geographical names in different formats, sometimes even more than one. We are going to extract them from the column using `SpaCy` english language model - \"en_core_web_sm\".\n",
    "\n",
    "We have already seen named-entity recognition in the Notebook2, so let's apply it here. We will extract all geographical entities (GPE). Let's see the example below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text=\"San Mateo, CA & Las Vegas, NV\"\n",
    "NLP = en_core_web_sm.load()\n",
    "output = NLP(text)\n",
    "for item in output.ents:\n",
    "    print(item.label_, item)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's create a function, that given a text from the \"tweet_location\" column returns all found GPE names. If the column field is empty, the function will return \"nolocationplaceholder\" to consistently specify the lack of the location."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def filter_location(text):\n",
    "    if text == \"\":\n",
    "        return \"nolocationplaceholder\"\n",
    "    else:\n",
    "        try:\n",
    "            output = NLP(text)\n",
    "            locations = []\n",
    "            for item in output.ents:\n",
    "                if item.label_ == \"GPE\":\n",
    "                    locations.append(str(item))\n",
    "            if not len(locations):\n",
    "                return \"nolocationplaceholder\"\n",
    "            return locations\n",
    "        except:\n",
    "            return \"nolocationplaceholder\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a special series in the dataframe for the extracted locations list (or nolocationplaceholder)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"location_ner\"] = df.tweet_location.swifter.apply(filter_location)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, we would like to have each extracted location in a specific column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "location_df = df[\"location_ner\"].apply(pd.Series)\n",
    "location_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now some concatenation and renaming..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.concat([location_df, df], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.rename(columns={0: 'loc_1', 1: 'loc_2', 2: \"loc_3\", 3: \"loc_4\"}, inplace=\"True\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And done!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So what can we do with these extracted location names? We can try to find a relation between the location name and the sentiment of the tweet. Firstly, let's see top 20 locations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dict(df.loc_1.value_counts().iloc[:20])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's see if there is any relation between tweets sentiment and the location. We can "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline_sentiment\",kind=\"count\", data=df[df.loc_1 == \"London\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline_sentiment\",kind=\"count\", data=df[df.loc_1 == \"San Francisco\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline_sentiment\",kind=\"count\", data=df[df.loc_1 == \"New York\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like there is no big difference from where the tweet was tweeted since the majority seems to be negative. However, let's take loc_1 as a feature and concatinate it with the tweet text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"text_loc\"] = df[\"text\"].astype(str) + \" \" + df[\"loc_1\"].astype(str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, now, we can drop all other unused columns and just keep our edited text and the airline_sentiment label."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df[[\"text_loc\", \"airline_sentiment\" ]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing and extracting features from text\n",
    "Now since we have already extracted the precise location of the tweets (if it was an actual geographical location) we can start with the preprocessing of the raw text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Language\n",
    "\n",
    "Firtly, we will start with determining the actuall language of each tweet. The fact that 10 tweets from the top and the bottom are in English doesn't mean that all of them are! We don't want to mix languages since words often repeat in many languages and they may have different meaning. To detect language of each tweet we will use the `langdetect` package."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['language'] = df[\"text_loc\"].swifter.apply(detect)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.language.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[df.language ==\"it\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[df.language ==\"fr\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like the langdetect was mistaken and the majority of all tweets is actually in English, so **let's not delete anything**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Emojis\n",
    "\n",
    "One interesting text-based feature that can be extracted is the occurence of emojis with the negative/positive meaning. The intuition behind this is to develop a list of negative emojis and if a tweet contains one of them, mark that tweet with a certain token, that the classifier will (hopefully) relate with the negative sentiment. However, before treating it as a feature we should check if it is related with the sentiment.\n",
    "\n",
    "To start, we need a way of extracting emojis from text. Let's explore the example below. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import emoji\n",
    "from emoji import UNICODE_EMOJI\n",
    "import re\n",
    "\n",
    "example_emoji_sentence = \"@VirginAmerica 👍 Need to start flying to @KCIAirport .  😊😀😃😄\"\n",
    "\n",
    "def extract_emojis(s):\n",
    "    return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "extract_emojis(example_emoji_sentence)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We may want to explore what kind of emojis and how joined together occur in our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emojis_ = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus = df.text_loc.tolist()\n",
    "for i in corpus:\n",
    "    h = extract_emojis(i)\n",
    "    if len(h) != 0:\n",
    "        emojis_.append(h)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# emojis_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a list of emojis with negative meaning and develop a function which will return the \"negemoji\" token if there is a negative emoji in the tweet text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "neg_emojis = \"😭,🆘, 😡, 😩, 😞, 😢, 👿, 👎, 😔, 😪, 😫, 😤, 😖, 😠, 💩, 😑, 😕, 😒\"\n",
    "\n",
    "def contains_neg_emoji(text):\n",
    "    emojis =  ''.join(c for c in text if c in emoji.UNICODE_EMOJI['en'])\n",
    "    contains_neg = \"\"\n",
    "    for e in emojis:\n",
    "        if e in neg_emojis:\n",
    "            contains_neg = \"negemoji\"\n",
    "            \n",
    "    return contains_neg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"neg_emoji\"] = df.text_loc.apply(contains_neg_emoji)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, now let's see how many tweets actually have negative emojis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.neg_emoji.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Oops, only 1% of all tweets contain negative emojis!  Let's check the relatin between negative emojis and the sentiment of the tweet."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(\"neg_emoji\", data=df[df[\"neg_emoji\"] == 'negemoji'], hue=\"airline_sentiment\", kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, the relation between containing negative emoji and the sentiment of a tweet looks very good - there is a very small number of positive-labeled tweets containing a negative emoji. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The occurence of negative emojis probably won't help much the classifier since it occurs in only 1% of all tweets but let's still implement it as a feature!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"text_loc\"] = df[\"text_loc\"].astype(str) + \" \" + df[\"neg_emoji\"].astype(str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df[[\"text_loc\", \"airline_sentiment\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further feature extraction - urls, numbers, hashtags...\n",
    "\n",
    "Here, we are going to exract more features from text. If we research that a specific feature discriminates well (is related to the positive or negative sentiment), we will mark a tweet with a specific token - as previously."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## URLS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "def contains_link(text):\n",
    "    text = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr',\n",
    "                     text)\n",
    "    text = word_tokenize(text)\n",
    "    if \"httpaddr\" in text:\n",
    "        return \"YES\"\n",
    "    else:\n",
    "        return \"NO\"\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"contains_link\"] = df.text_loc.apply(contains_link)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.factorplot(\"contains_link\", data=df, hue=\"airline_sentiment\", kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Number"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def contains_number(text):\n",
    "        \n",
    "    text = re.sub(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'numbr',\n",
    "                     text)\n",
    "    \n",
    "    text = re.sub(r'\\d+(\\.\\d+)?', 'numbr',\n",
    "                     text)\n",
    "    text = word_tokenize(text)\n",
    "    if \"numbr\" in text:\n",
    "        return \"YES\"\n",
    "    else:\n",
    "        return \"NO\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"contains_number\"] = df.text_loc.apply(contains_number)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.factorplot(\"contains_number\", data=df, hue=\"airline_sentiment\", kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HASHTAG"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def contains_hashtag(text):\n",
    "        \n",
    "    text = re.sub(r'\\b#w+\\b', 'hashtg',\n",
    "                     text)\n",
    "    \n",
    "    text = re.sub(r'\\d+(\\.\\d+)?', 'hashtg',\n",
    "                     text)\n",
    "    text = word_tokenize(text)\n",
    "    if \"hashtg\" in text:\n",
    "        return \"YES\"\n",
    "    else:\n",
    "        return \"NO\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "df[\"contains_hashtag\"] = df.text_loc.apply(contains_hashtag)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "sns.factorplot(\"contains_hashtag\", data=df, hue=\"airline_sentiment\", kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sadly, the occurence of URLs, numbers or hashtags doesn't seem to have a strong influence on the tweet sentiment. We will not add them as features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing - cleaning\n",
    "\n",
    "Now we need to clean the data. Here, we will replace all urls, numbers with a standard token. Beside, we are going to remove all punctuation so all mentions and hashtags will become normal words. Beside, we will make all tweets lowercase."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "# After our quick reserach location also doesn't seem to have \n",
    "# a strong influence so we will remove the \"nolocationplaceholder\" token as well.\n",
    "stop_words.append(\"nolocationplaceholder\")\n",
    "\n",
    "# Convert to set, so lookup operations are much faster\n",
    "stop_words = set(stop_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    text = re.sub(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', ' numbr ',\n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'\\d+(\\.\\d+)?', ' numbr ',\n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', ' httpaddr ',\n",
    "                  text)\n",
    "\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    # convert to lower case and split\n",
    "    words = word_tokenize(letters_only_text.lower())\n",
    "\n",
    "    # remove stopwords\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "    sen = ' '.join(cleaned_words)\n",
    "\n",
    "    return sen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"cleaned_text\"] =df.text_loc.swifter.apply(preprocess)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## REMOVING RARE WORDS\n",
    "\n",
    "If a word occurs once or twice in the whole dataset, its occurence doesn't change much. However, since these words are still \"analyzed\" by the classifier, they may infact violate the classification process! It is common to remove such words. Let's start with creating a dictionary of the word counts - basically a term frequency dictionary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "app_desc_clean_list = df[\"cleaned_text\"].tolist()\n",
    "whole_corpus = \" \".join(app_desc_clean_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "rare_words = {} # again, storing it in dictionary instead of list will make looking through it much faster\n",
    "\n",
    "counter_dic = Counter(whole_corpus.split())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define word as \"rare\", if it occurs only once or twice in the corpus."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for (key,value) in counter_dic.items():\n",
    "    if value < 3:\n",
    "        rare_words[key] = value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's explore which words are actually rare."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rare_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(rare_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(set(corpus))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can delete them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def delete_rare_words(text):\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in rare_words:\n",
    "            cleaned_words.append(word)\n",
    "            \n",
    "    return ' '.join(cleaned_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].swifter.apply(delete_rare_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One more thing - if there are any duplicates let's delete them as well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df.drop_duplicates()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Change textual labels into numeric values (PROCESSING)\n",
    "\n",
    "And the very last thing we need to do before training classifiers, is to encode text labels into numbers. Some `sklearn` classifiers like mulitnomial Logistic Regression do not accept textual labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "possible_labels = df.airline_sentiment.unique()\n",
    "possible_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "\n",
    "# We can refer to this mapping later\n",
    "label_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['label'] = df.airline_sentiment.replace(label_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end, let's select only important Series."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df[[\"cleaned_text\", \"airline_sentiment\", \"label\"]]\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready for the final confrontation!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing with baseline\n",
    "\n",
    "Firstly, let's use the same classifier and its configuration, so we can see how our processing and feature engineering changed the accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_text, df.airline_sentiment, test_size=0.1, \n",
    "                                                                                random_state=40, stratify=df.airline_sentiment)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1),binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mnb = MultinomialNB()\n",
    "y_pred = mnb.fit(X_train_counts, y_train)\n",
    "y_predicted_counts = mnb.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(mnb, X_train_counts, y_train, cv=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#old score: 0.7596467191733065\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#different vectorizer\n",
    "\n",
    "count_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "y_pred = mnb.fit(X_train_counts, y_train)\n",
    "y_predicted_counts = mnb.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(mnb, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "so we will stick to countvectorizer because it gives us different results. lets play around with the parameters of the vectorizer before we tweak the classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,1) ,binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "y_pred = mnb.fit(X_train_counts, y_train)\n",
    "y_predicted_counts = mnb.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(mnb, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "increasing n-gram range also doesnt increase the performance of the classifier. so lets try out another classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2) ,binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = LogisticRegression(solver=\"liblinear\") #(C=2.3, class_weight='balanced',solver='newton-cg',multi_class='multinomial',random_state=10)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "nice - logistic regression performs bsignificantly better (you can even increase ngram range and performance slighly increaes). lets do a grid search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'C': np.linspace(0.1, 1, 5), \"solver\":[\"newton-cg\"]}#, \"penalty\":[\"l1\",\"l2\"], \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]}\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "grid_search.fit(X_train_counts, y_train)\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = LogisticRegression(C=0.325, solver='newton-cg' ) #(C=2.3, class_weight='balanced',solver='newton-cg',multi_class='multinomial',random_state=10)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Increasing ngram range doesnt make a difference, so one could either play around more with the parameters or try out different classifiers. lets try our last one: SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = svm.SVC(kernel=\"poly\", degree=2)\n",
    "\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {'C': np.linspace(0.1, 5, 5), \"kernel\":[\"rbf\"]}#, \"penalty\":[\"l1\",\"l2\"], \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]}\n",
    "grid_search = GridSearchCV(svm.SVC(), parameters)\n",
    "grid_search.fit(X_train_counts, y_train)\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#increasing training set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_text, df.airline_sentiment, test_size=0.1, \n",
    "                                                                                random_state=40, stratify=df.airline_sentiment)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1),binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(C=0.325, solver='newton-cg' ) #(C=2.3, class_weight='balanced',solver='newton-cg',multi_class='multinomial',random_state=10)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean()  #no big difference - overfitting on training set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "it seems like with linear classifiers we will not get much more than 78% accuracy out of it. of course one should also look at the missclassification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf, X_test_counts, y_test) \n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_df = pd.DataFrame({\"text\": X_test, \"real target\": y_test, \"predicted target\": y_predicted_counts})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wrong = y_df[y_df[\"real target\"] != y_df[\"predicted target\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wrong.to_csv(\"wrong_predictions_airline.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#what happens if we deete the airline info from the tweets so that they dont ifluence them?\n",
    "stop_words.extend([\"united\", \"virginamerica\", \"united\", \"delta\", \"usairways\", \"americanair\", \"southwestair\", \"jetblue\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"cleaned_text\"] =df.text_loc.swifter.apply(preprocess)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_text, df.airline_sentiment, test_size=0.2, \n",
    "                                                                                random_state=40, stratify=df.airline_sentiment)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2),binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(C=0.325, solver='newton-cg' ) #(C=2.3, class_weight='balanced',solver='newton-cg',multi_class='multinomial',random_state=10)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean()  #no big difference - overfitting on training set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf, X_test_counts, y_test) \n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "so now the classifier makes fewer mislabels as neutral if something was negative but mislabels neutrals more often as negative. however, as we can see, the airline has no significant impact on the sentiment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_df = pd.DataFrame({\"text\": X_test, \"real target\": y_test, \"predicted target\": y_predicted_counts})\n",
    "wrong = y_df[y_df[\"real target\"] != y_df[\"predicted target\"]]\n",
    "wrong.to_csv(\"wrong_predictions_airline.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def polarity_tb(text):\n",
    "    text = TextBlob(text)\n",
    "    pol = text.sentiment.polarity\n",
    "    return pol\n",
    "\n",
    "df[\"polarity\"] = df.text_loc.swifter.apply(polarity_tb)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[df.airline_sentiment == \"neutral\"].polarity.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def translate_polarity(pol):\n",
    "    if pol < 0.1 and pol> -0.1:\n",
    "        return \"neutral\"\n",
    "    \n",
    "    if pol >= 0.1:\n",
    "        return \"positive\"\n",
    "    \n",
    "    if pol <= -0.1:\n",
    "        return \"negative\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"polarity_tb\"] = df.polarity.apply(translate_polarity)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tb_wrong = df[df[\"airline_sentiment\"] != df[\"polarity_tb\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(tb_wrong)/len(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.polarity_tb.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "as we can see textblob doesnt do a great job when classifying the sentiment as it tends to label tweets as neutran and positive rather than negative"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}