{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 6 - Putting it all together\n",
    "\n",
    "Although in this notebook we will not go over new topics, we are going to see how we can fully handle a text sentiment classification task. We are going to use many different techniques covered in previous notebooks, that (hopefully) let us develop quite an efficient model. Additionally, we are going to create some visualizations of dataset statistics and model performance, so we can understand the problem better.\n",
    "\n",
    "We are going to use the dataset containing tweets about US airlines from [kaggle](). The dataset is stored in the CSV file `airline_tweets.csv` and contains tweet text labeled with either *\"positive\"*, *\"neutral\"* or *\"negative\"* sentiment. Besides, it contains (potentially) helpful information like the name of the airline and the user’s location. Let's explore it!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import swifter\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, TfidfTransformer\n",
    "from langdetect import detect\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import en_core_web_sm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing & feature engineering\n",
    "\n",
    "## Dataset overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_tweets = pd.read_csv(\"./datasets/airline_tweets.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_tweets.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, so there are many columns with additional information. However, some of their fields are empty. To determine whether we want to include a certain column in the further analysis, we may want to inspect how many fields in these columns are null. Let's do it using the `.info()` method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_tweets.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, now we have an overview of the dataset and we can see that columns like `airline_sentiment_gold` or `tweet_coord` are generally empty (contain null values). How about columns like `tweet_location` or `user_timezone`? We can inspect them visually and see if \"null\" values are stacked or randomly scattered throughout the whole dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.heatmap(df_tweets.isnull(), cbar=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our study, besides \"text\" and \"airline_sentiment\" we will use the \"airline\" and \"tweet_location\" columns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df_tweets[[ \"airline_sentiment\", \"airline\", \"text\", \"tweet_location\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline classification on raw data\n",
    "\n",
    "Now, let's train the Naive-Bayes classifier without any preprocessing or parameter tuning, and let's see what accuracy score we get."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tweets.text, df_tweets.airline_sentiment, test_size=0.2, \n",
    "                                                                                random_state=40)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1),binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_counts, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_predicted_counts = mnb.predict(X_test_counts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None, average='weighted')             \n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like even without much work we can get not bad results! Let's verify them using cross-validation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(mnb, X_train_counts, y_train, cv=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, so the baseline classifier ran on the totally unprocessed dataset gives us **around 75% accuracy** using cross-validation. We will come back to this number later!\n",
    "\n",
    "From now we aim to get a higher accuracy score. We are going to preprocess data, extract some features and work on the model parameters selection, so all of this work will hopefully let us get better results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further dataset exploration\n",
    "\n",
    "Let's see what is the structure of the dataset in terms of the airline and sentiment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline\",kind=\"count\", data=df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In terms of airlines, the situation doesn't look very bad except for the low number of tweets regarding Virgin America."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline_sentiment\", kind=\"count\", data=df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, when it comes to sentiment, we can see that the dataset is highly imbalanced. We have much more negative tweets than neutral or positive ones. Often the result of such imbalance is the lower accuracy. To partially solve this issue we may want to *resample* the dataset later.\n",
    "\n",
    "We may also want to see what is the sentiment statistic for each airline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.factorplot(\"airline\", data=df, hue=\"airline_sentiment\", kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What we can read from this plot? Actually quite useful information! We can see that the airline people tweet about has an influence on the tweet sentiment. For example, the great majority of all tweets about United Airlines are negative. On the other hand, neutral and positive tweets regarding Delta Airlines outnumber negative samples. Hence, we will want to treat the tweet-related airline as a feature."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature extraction - airline \n",
    "\n",
    "So how can we use the name of the airline as a feature? One way is to add it at the end of the text. By doing so, we will give a classifier a \"hint\", that is related to the sentiment. Interestingly, since every tweet mentions one of 6 airlines, they already contain airlines names in the beginning! Let's make sure that it is true for all tweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_starting_word = df[\"text\"].apply(word_tokenize)\n",
    "pd.Series(x[1].lower() for x in df_starting_word).value_counts().head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the great majority of all tweets start with the airline name. Some tweets do not, but we will ignore them since these are rare cases. In the preprocessing, we will remove \"@\" characters and make all words lowercase so we will treat \"united\" and \"United\" as the same token.\n",
    "\n",
    "Hence, we can drop the \"airline\" column, since we will not use it anymore."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del df[\"airline\"]\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature extraction - location\n",
    "\n",
    "We can also extract the location from the \"tweet_location\" column. One may think that since the column already exists, the only thing we need to do is to match it with the label column. In this case, this won't work, since the \"tweet_location\" column contains geographical names in different formats, sometimes contains even more than one location. We are going to extract them from the column using the `SpaCy` English language model - \"en_core_web_sm\" (already downloaded in the Notebook 3).\n",
    "\n",
    "We have already seen named-entity recognition in Notebook 2, so let's apply it here. We will extract all geographical entities (GPE). Let's see the example below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text=\"San Mateo, CA & Las Vegas, NV\"\n",
    "NLP = en_core_web_sm.load()\n",
    "output = NLP(text)\n",
    "for item in output.ents:\n",
    "    print(item.label_, item)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's create a function, that given a text from the \"tweet_location\" column returns all found GPE names. If the column field is empty, the function will return \"nolocationplaceholder\" to consistently specify the lack of the location."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def filter_location(text):\n",
    "    if text == \"\":\n",
    "        return \"nolocationplaceholder\"\n",
    "    else:\n",
    "        try:\n",
    "            output = NLP(text)\n",
    "            locations = []\n",
    "            for item in output.ents:\n",
    "                if item.label_ == \"GPE\":\n",
    "                    locations.append(str(item))\n",
    "            if not len(locations):\n",
    "                return \"nolocationplaceholder\"\n",
    "            return locations\n",
    "        except:\n",
    "            return \"nolocationplaceholder\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a special series in the dataframe for the extracted locations list (or nolocationplaceholder)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"location_ner\"] = df.tweet_location.swifter.apply(filter_location)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, we would like to have each extracted location in a specific column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "location_df = df[\"location_ner\"].apply(pd.Series)\n",
    "location_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now some concatenation and renaming..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.concat([location_df, df], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.rename(columns={0: 'loc_1', 1: 'loc_2', 2: \"loc_3\", 3: \"loc_4\"}, inplace=\"True\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And done!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So what can we do with these extracted location names? We can try to find a relation between the location name and the sentiment of the tweet. Firstly, let's see the top 20 locations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dict(df.loc_1.value_counts().iloc[:20])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's see if there is any relation between tweets sentiment and the location. We can "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline_sentiment\",kind=\"count\", data=df[df.loc_1 == \"London\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline_sentiment\",kind=\"count\", data=df[df.loc_1 == \"San Francisco\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline_sentiment\",kind=\"count\", data=df[df.loc_1 == \"New York\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like there is no big difference from where the tweet was tweeted since the majority seems to be negative. However, let's take loc_1 as a feature and concatenate it with the tweet text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"text_loc\"] = df[\"text\"].astype(str) + \" \" + df[\"loc_1\"].astype(str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, now, we can drop all other unused columns and just keep our edited text and the airline_sentiment label."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df[[\"text_loc\", \"airline_sentiment\" ]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing and extracting features from text\n",
    "Now since we have already extracted the precise location of the tweets (if it was an actual geographical location) we can start with the preprocessing of the raw text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Language\n",
    "\n",
    "Firstly, we will start with determining the actual language of each tweet. The fact that 10 tweets from the top and the bottom are in English doesn't mean that all of them are! We don't want to mix languages since the same words often repeat in different languages and may have different meanings. To detect the language of each tweet we will use the `langdetect` package."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['language'] = df[\"text_loc\"].swifter.apply(detect)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.language.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[df.language ==\"it\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[df.language ==\"fr\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like the langdetect was mistaken and the majority of all tweets is actually in English, so **let's not delete anything**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Emojis\n",
    "\n",
    "One interesting text-based feature that can be extracted is the occurrence of emojis with a negative/positive meaning. The intuition behind this is to develop a list of negative emojis and if a tweet contains one of them, mark that tweet with a certain token, that the classifier will (hopefully) relate with the negative sentiment. However, before treating it as a feature we should check if it is related to the sentiment.\n",
    "\n",
    "To start, we need a way of extracting emojis from the text. Let's explore the example below. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import emoji\n",
    "from emoji import UNICODE_EMOJI\n",
    "import re\n",
    "\n",
    "example_emoji_sentence = \"@VirginAmerica 👍 Need to start flying to @KCIAirport .  😊😀😃😄\"\n",
    "\n",
    "def extract_emojis(s):\n",
    "    return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "extract_emojis(example_emoji_sentence)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We may want to explore what kind of emojis and how joined together occur in our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emojis_ = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus = df.text_loc.tolist()\n",
    "for i in corpus:\n",
    "    h = extract_emojis(i)\n",
    "    if len(h) != 0:\n",
    "        emojis_.append(h)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emojis_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a list of emojis with negative meanings and develop a function that will return the \"negemoji\" token if there is a negative emoji in the tweet text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "neg_emojis = \"😭,🆘, 😡, 😩, 😞, 😢, 👿, 👎, 😔, 😪, 😫, 😤, 😖, 😠, 💩, 😑, 😕, 😒\"\n",
    "\n",
    "def contains_neg_emoji(text):\n",
    "    emojis =  ''.join(c for c in text if c in emoji.UNICODE_EMOJI['en'])\n",
    "    contains_neg = \"\"\n",
    "    for e in emojis:\n",
    "        if e in neg_emojis:\n",
    "            contains_neg = \"negemoji\"\n",
    "            \n",
    "    return contains_neg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"neg_emoji\"] = df.text_loc.apply(contains_neg_emoji)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, now let's see how many tweets actually have negative emojis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.neg_emoji.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Oops, only 1% of all tweets contain negative emojis!  Let's check the relation between negative emojis and the sentiment of the tweet."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(\"neg_emoji\", data=df[df[\"neg_emoji\"] == 'negemoji'], hue=\"airline_sentiment\", kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, the relation between containing negative emoji and the sentiment of a tweet looks very good - there is a very small number of positive-labeled tweets containing a negative emoji. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The occurrence of negative emojis probably won't help much the classifier since it occurs in only 1% of all tweets but let's still implement it as a feature!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"text_loc\"] = df[\"text_loc\"].astype(str) + \" \" + df[\"neg_emoji\"].astype(str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df[[\"text_loc\", \"airline_sentiment\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further feature extraction - urls, numbers, hashtags...\n",
    "\n",
    "Here, we are going to extract more features from the text. If we research that a specific feature discriminates well (is related to the positive or negative sentiment), we will mark a tweet with a specific token - as previously."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### URLs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "def contains_link(text):\n",
    "    text = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr',\n",
    "                     text)\n",
    "    text = word_tokenize(text)\n",
    "    if \"httpaddr\" in text:\n",
    "        return \"YES\"\n",
    "    else:\n",
    "        return \"NO\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"contains_link\"] = df.text_loc.apply(contains_link)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.factorplot(\"contains_link\", data=df, hue=\"airline_sentiment\", kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Numbers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def contains_number(text):\n",
    "        \n",
    "    text = re.sub(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'numbr',\n",
    "                     text)\n",
    "    \n",
    "    text = re.sub(r'\\d+(\\.\\d+)?', 'numbr',\n",
    "                     text)\n",
    "    text = word_tokenize(text)\n",
    "    if \"numbr\" in text:\n",
    "        return \"YES\"\n",
    "    else:\n",
    "        return \"NO\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"contains_number\"] = df.text_loc.apply(contains_number)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.factorplot(\"contains_number\", data=df, hue=\"airline_sentiment\", kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hashtags"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def contains_hashtag(text):\n",
    "        \n",
    "    text = re.sub(r'\\b#w+\\b', 'hashtg',\n",
    "                     text)\n",
    "    \n",
    "    text = re.sub(r'\\d+(\\.\\d+)?', 'hashtg',\n",
    "                     text)\n",
    "    text = word_tokenize(text)\n",
    "    if \"hashtg\" in text:\n",
    "        return \"YES\"\n",
    "    else:\n",
    "        return \"NO\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"contains_hashtag\"] = df.text_loc.apply(contains_hashtag)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.factorplot(\"contains_hashtag\", data=df, hue=\"airline_sentiment\", kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sadly, the occurrence of URLs, numbers, or hashtags doesn't seem to have a strong influence on the tweet sentiment. We will not add them as features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing - cleaning\n",
    "\n",
    "### Standard preprocessing \n",
    "\n",
    "Now we need to clean the data. Here, we will replace all URLs, numbers with a standard token. Besides, we are going to remove all punctuation so all mentions and hashtags will become normal words and make all tweets lowercase."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "# After our quick reserach location also doesn't seem to have \n",
    "# a strong influence so we will remove the \"nolocationplaceholder\" token as well.\n",
    "stop_words.append(\"nolocationplaceholder\")\n",
    "\n",
    "# Convert to set, so lookup operations are much faster\n",
    "stop_words = set(stop_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    text = re.sub(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', ' numbr ',\n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'\\d+(\\.\\d+)?', ' numbr ',\n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', ' httpaddr ',\n",
    "                  text)\n",
    "\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    # convert to lower case and split\n",
    "    words = word_tokenize(letters_only_text.lower())\n",
    "\n",
    "    # remove stopwords\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "    sen = ' '.join(cleaned_words)\n",
    "\n",
    "    return sen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"cleaned_text\"] =df.text_loc.swifter.apply(preprocess)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rare word removal\n",
    "\n",
    "If a word occurs once or twice in the whole dataset, its occurrence doesn't change much. However, since these words are still \"analyzed\" by the classifier, they may in fact violate the classification process! It is common to remove such words. Let's start with creating a dictionary of the word counts - basically a term frequency dictionary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "app_desc_clean_list = df[\"cleaned_text\"].tolist()\n",
    "whole_corpus = \" \".join(app_desc_clean_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "rare_words = {} # again, storing it in dictionary instead of list will make looking through it much faster\n",
    "\n",
    "counter_dic = Counter(whole_corpus.split())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define a word as \"rare\" if it occurs only once or twice in the corpus."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for (key,value) in counter_dic.items():\n",
    "    if value < 3:\n",
    "        rare_words[key] = value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's explore which words are actually \"rare\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rare_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(rare_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(set(corpus))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can delete them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def delete_rare_words(text):\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in rare_words:\n",
    "            cleaned_words.append(word)\n",
    "            \n",
    "    return ' '.join(cleaned_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].swifter.apply(delete_rare_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One more thing - if there are any duplicates let's delete them as well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df.drop_duplicates()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the other hand, we may also want to see the most frequent words. Let's use the `wordcloud` to visualize it!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Create a list of word\n",
    "text=whole_corpus\n",
    " \n",
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=480, height=480, background_color='white').generate(text)\n",
    " \n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud,  interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Textual labels conversion to numeric values\n",
    "\n",
    "And the very last to do before training classifiers is to encode text labels into numeric labels. Some classifiers may not accept textual labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "possible_labels = df.airline_sentiment.unique()\n",
    "possible_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "\n",
    "# We can refer to this mapping later\n",
    "label_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['label'] = df.airline_sentiment.replace(label_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Latent Semantic Analysis (LSA) visualization\n",
    "\n",
    "Latent Semantic Analysis is a tool for grouping documents with similar meanings. We can use it to visualize the current dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.cleaned_text, df.label, test_size=0.2, random_state=40)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1), binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n",
    "        lsa = TruncatedSVD(n_components=2)\n",
    "        lsa.fit(test_data)\n",
    "        lsa_scores = lsa.transform(test_data)\n",
    "        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n",
    "        color_column = [color_mapper[label] for label in test_labels]\n",
    "        colors = ['orange','green', \"red\"]\n",
    "        if plot:\n",
    "            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "            orange_patch = mpatches.Patch(color='orange', label='Neutral')\n",
    "            blue_patch = mpatches.Patch(color='green', label='Positive')\n",
    "            green_patch = mpatches.Patch(color='red', label='Negative')\n",
    "            plt.legend(handles=[orange_patch, blue_patch, green_patch], prop={'size': 20})\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))          \n",
    "plot_LSA(X_train_counts, y_train)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready for the final confrontation!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model training\n",
    "\n",
    "In this section, we are going to train and evaluate different classifiers. Keep in mind that the dataset is very messy and we shouldn't expect astonishing performance improvement."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing with baseline\n",
    "\n",
    "Firstly, let's use the same classifier and its configuration, so we can see how our processing and feature engineering changed the accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.cleaned_text, df.airline_sentiment, test_size=0.2, random_state=40)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1), binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mnb = MultinomialNB()\n",
    "y_pred = mnb.fit(X_train_counts, y_train)\n",
    "y_predicted_counts = mnb.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(mnb, X_train_counts, y_train, cv=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#old score: 0.7596467191733065"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok... using the same classifier (and its parameters) as before, we have a slight improvement of 0.5%. This is definitely not sufficient and we want more! \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using different vectorizer\n",
    "\n",
    "Let's consider a different vectorizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "y_pred = mnb.fit(X_train_counts, y_train)\n",
    "y_predicted_counts = mnb.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(mnb, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like the TD-IDF doesn't work well in this case. We will stick to the countvectorizer because it gives us a bit better results. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modifying vectorizer parameters\n",
    "\n",
    "Let's play around with the parameters of the vectorizer before we tweak the classifier and increase the n-gram range. N-grams are joints of n-consecutive words. The range specifies the minimum and the maximum number of words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,3) ,binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "y_pred = mnb.fit(X_train_counts, y_train)\n",
    "y_predicted_counts = mnb.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(mnb, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Increasing the n-gram range also doesn't increase the performance of the classifier. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Logistic Regression\n",
    "\n",
    "Let's try out another classifier - Logistic Regression."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2) ,binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = LogisticRegression(solver=\"newton-cg\")\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice - logistic regression performs significantly better. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Searching for best hyper-parameters\n",
    "\n",
    "As you know, there are hyper-parameters like the `C` parameter, `solver` or `penalty`. We can try to find the best parameters manually, but it is much better to perform the Grid Search. Let's run it! **Note: This may take a lot of time!**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'C': np.linspace(0.1, 1, 5), \"solver\":[\"newton-cg\"]}\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "grid_search.fit(X_train_counts, y_train)\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scores: ', grid_search.best_score_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = LogisticRegression(C=0.55, solver='newton-cg' )\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM\n",
    "\n",
    "We will use several configurations for SVM."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = svm.SVC(kernel=\"poly\", degree=2)\n",
    "\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, we can try to find best parameters for SVM using the Grid Search."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {'C': np.linspace(0.1, 5, 5), \"kernel\":[\"rbf\"]} \n",
    "grid_search = GridSearchCV(svm.SVC(), parameters)\n",
    "grid_search.fit(X_train_counts, y_train)\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-test split modification\n",
    "Another approach is to modify the train-test split. \n",
    "\n",
    "### Size changes\n",
    "Let's make the training set larger and add the stratify parameter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# increasing training set size\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_text, df.airline_sentiment, test_size=0.1,\n",
    "                                                    random_state=40)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), binary=True)\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(C=0.325, solver='newton-cg')\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" %\n",
    "      (accuracy, precision, recall, f1))\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean()  # no big difference - small overfitting on training set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resampling\n",
    "We can also try to randomly oversample positive and neutral tweets to create a more balanced training dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_text, df.airline_sentiment, test_size=0.2,\n",
    "                                                    random_state=40)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='auto')\n",
    "\n",
    "# Only resample the training dataset\n",
    "X_train_over, y_train_over = oversample.fit_resample(X_train.to_numpy().reshape(-1,1), y_train)\n",
    "X_train_over = X_train_over.T[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before resampling..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline_sentiment\", kind=\"count\", data=pd.DataFrame({\"text\" : pd.Series(X_train), \"airline_sentiment\" : pd.Series(y_train)}))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And after..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"airline_sentiment\", kind=\"count\", data=pd.DataFrame({\"text\" : pd.Series(X_train_over), \"airline_sentiment\" : pd.Series(y_train_over)}))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train_counts = count_vectorizer.fit_transform(X_train_over)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(C=0.325, solver='newton-cg')\n",
    "clf.fit(X_train_counts, y_train_over)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" %\n",
    "      (accuracy, precision, recall, f1))\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf, X_train_counts, y_train_over, cv=5, scoring=\"f1_weighted\")\n",
    "scores.mean()  # high overfitting on training set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The test set result is didn't change much but as you can see, there is **very high overfitting** when running CV on the training set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "Let's say this is our final model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_text, df.airline_sentiment, test_size=0.2,\n",
    "                                                    random_state=40)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), binary=True)\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(C=0.325, solver='newton-cg')\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like with linear classifiers we will not get much more than 78% accuracy out of it. \n",
    "\n",
    "### Confusion matrix\n",
    "Of course one should also look at the misclassification. We will use the 3x3 confusion matrix to get some insights."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf, X_test_counts, y_test) \n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Words with strong influence\n",
    "\n",
    "Let's inspect which particular words have the highest influence on the classification decision for each class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weights_dict = {}\n",
    "weights_dict[\"feature\"] = count_vectorizer.get_feature_names()\n",
    "for class_num in range(3):\n",
    "    weights_dict[clf.classes_[class_num]] = clf.coef_[class_num]\n",
    "\n",
    "features_weights = pd.DataFrame(weights_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features_weights.sort_values(by=\"negative\", ascending=False).head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features_weights.sort_values(by=\"neutral\", ascending=False).head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features_weights.sort_values(by=\"positive\", ascending=False).head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Further ideas\n",
    "\n",
    "However, if you are brave enough to continue working with this dataset..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What happens if we delete the airline info from the tweets so that they don't influence them?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stop_words = list(stop_words)\n",
    "# What is interesting, even that tweets are not about \"jetblue\" airlines, \n",
    "# it still occurs quite often in the dataset so let's remove it as well.\n",
    "stop_words.extend([\"virginamerica\", \"united\", \"delta\", \"usairways\", \"americanair\", \"southwestair\", \"jetblue\"])  \n",
    "stop_words = set(stop_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_no_airlines = df.copy(deep=True)\n",
    "df_no_airlines[\"cleaned_text\"] =df_no_airlines.text_loc.swifter.apply(preprocess)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize what are the most frequent words in the dataset after this change."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "app_desc_clean_list = df_no_airlines[\"cleaned_text\"].tolist()\n",
    "whole_corpus = \" \".join(app_desc_clean_list)\n",
    "\n",
    "# Create a list of word\n",
    "text=whole_corpus\n",
    " \n",
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=480, height=480, background_color='white').generate(text)\n",
    " \n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud,  interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_no_airlines.cleaned_text, df_no_airlines.airline_sentiment, test_size=0.2,\n",
    "                                                    random_state=40)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), binary=True)\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = LogisticRegression(C=0.325, solver='newton-cg' )\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf, X_test_counts, y_test) \n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So now the classifier makes fewer mislabels as neutral if something was negative but mislabels neutrals more often as negative. Although it has better accuracy on negatives, on positives the accuracy is lower. However, as we can see, the airline has no significant impact on the sentiment. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What happens if we change the problem into the binary classification (negative vs non-negative)?\n",
    "\n",
    "For the previous problem we have created a separate dataframe, so here we are going back to using airline tokens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Previous labels: 0 - neutral, 1 - positive, 2 - negative\n",
    "# New labels: 1 - non-negative, 2 negative\n",
    "df[\"binary_label\"] = df.label.apply(lambda x: 1 if x == 0 else x)\n",
    "df[\"binary_airline_sentiment\"] = df.airline_sentiment.apply(lambda x: \"positive\" if x == \"neutral\" else x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"binary_airline_sentiment\", kind=\"count\", data=df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the dataset is much less imbalanced. Although it could be downsampled or oversampled, we will not change it for now."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_text, df.binary_airline_sentiment, test_size=0.1, \n",
    "                                                                                random_state=40)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1),binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(C=0.325, solver='newton-cg')\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "82%! As you can see, even on the first attempt we can get a much higher score. However, we had to pay for it by removing a class and transforming the task into the binary decision. We can also try to visualize it using the t-SNE LSA graph."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_text, df.binary_label, test_size=0.2, \n",
    "                                                                                random_state=40)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1),binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n",
    "        lsa = TruncatedSVD(n_components=2)\n",
    "        lsa.fit(test_data)\n",
    "        lsa_scores = lsa.transform(test_data)\n",
    "        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n",
    "        color_column = [color_mapper[label] for label in test_labels]\n",
    "        colors = ['green', \"red\"]\n",
    "        if plot:\n",
    "            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "            blue_patch = mpatches.Patch(color='green', label='Positive')\n",
    "            green_patch = mpatches.Patch(color='red', label='Negative')\n",
    "            plt.legend(handles=[blue_patch, green_patch], prop={'size': 20})\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))          \n",
    "plot_LSA(X_train_counts, y_train)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What happens if we use descriptions of emojis as additional features?\n",
    "\n",
    "The `emoji` python package contains descriptions of each emoji. Since these descriptions often express emotions (\"**sad** face\", \"**happy** face\"), we can try to use them instead of marking tweets containing negative emojis. Of course, in our case emojis are present in an extremely small number of tweets, so the improvement won't be significant. However, in other applications, it may result in better performance. Let's see how it works with the following example."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def emojis_meaning(text):\n",
    "    result = \"\"\n",
    "    emojis =  ''.join(c for c in text if c in emoji.UNICODE_EMOJI['en'])\n",
    "    for emo in emojis:\n",
    "        result += re.sub(r'_', \" \", emoji.demojize(emo)[1:-1]) + \" \"\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emojis_meaning(\"😀👍\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emojis_meaning(\"@VirginAmerica 👍 Need to start flying to @KCIAirport .  😊😀😃😄\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['emojis_meaning'] = df.text_loc.apply(emojis_meaning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we need to use raw data again, I will paste data processing in the cell below. Nothing new, we've already done it above - this is all the processing we did, compressed in one cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df_tweets[[ \"airline_sentiment\", \"airline\", \"text\", \"tweet_location\"]]\n",
    "df[\"location_ner\"] = df.tweet_location.swifter.apply(filter_location)\n",
    "location_df = df[\"location_ner\"].apply(pd.Series)\n",
    "df = pd.concat([location_df, df], axis=1)\n",
    "df.rename(columns={0: 'loc_1', 1: 'loc_2', 2: \"loc_3\", 3: \"loc_4\"}, inplace=\"True\")\n",
    "df[\"text_loc\"] = df[\"text\"].astype(str) + \" \" + df[\"loc_1\"].astype(str)\n",
    "df = df[[\"text_loc\", \"airline_sentiment\" ]]\n",
    "df[\"neg_emoji\"] = df.text_loc.apply(emojis_meaning)  # change here to \"contains_neg_emoji\" if you want to use negative emojis again\n",
    "df[\"text_loc\"] = df[\"text_loc\"].astype(str) + \" \" + df[\"neg_emoji\"].astype(str)\n",
    "df = df[[\"text_loc\", \"airline_sentiment\"]]\n",
    "df[\"cleaned_text\"] =df.text_loc.swifter.apply(preprocess)\n",
    "app_desc_clean_list = df[\"cleaned_text\"].tolist()\n",
    "whole_corpus = \" \".join(app_desc_clean_list)\n",
    "rare_words = {}\n",
    "counter_dic = Counter(whole_corpus.split())\n",
    "for (key,value) in counter_dic.items():\n",
    "    if value < 3:\n",
    "        rare_words[key] = value\n",
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].swifter.apply(delete_rare_words)\n",
    "df = df.drop_duplicates()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train the classifier!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cleaned_text, df.airline_sentiment, test_size=0.1, \n",
    "                                                                                random_state=40, stratify=df.airline_sentiment)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2),binary=True)\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(C=0.325, solver='newton-cg' )\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf, X_train_counts, y_train, cv=5)\n",
    "scores.mean() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, it works a bit better, but since emojis are only in about 1% of all tweets, this modification doesn't have a strong effect on the classification.\n",
    "\n",
    "## This is the end of Notebook 6\n",
    "\n",
    "Feel free to experiment with this dataset more! If you are interested in the most efficient algorithms developed to deal with this problem, the airline classification dataset has been used by many researchers and you can find many papers about it online. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}