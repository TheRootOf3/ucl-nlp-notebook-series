{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 2 - Text preprocessing, POS tags, and simple word model\n",
    "\n",
    "So far we have learned how to read files, manage them using pandas data frames and search for patterns using regular expressions. In this notebook we will look into the next concepts:\n",
    "- text preprocessing\n",
    "- POS tagging & Named-entity recognition (NER)\n",
    "- bag-of-words model\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Text preprocessing\n",
    "Before running any analysis or developing a language model, we have to make sure that our data is in a suitable format, which will guarantee the best performance and accuracy of the algorithm. This step is called **text preprocessing** and consists of several smaller tasks.\n",
    "\n",
    "### 3.1 Load from csv to pandas\n",
    "In this section, we will use the same 'dummy_data' dataset which we used in the previous notebook. Firstly, let's load it!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dummy_data_dataset_file = \"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook2/datasets/dummy_data.csv\"\n",
    "\n",
    "''' uncomment if you want to run it locally '''\n",
    "# dummy_data_dataset_file = \"./datasets/dummy_data.csv\"\n",
    "\n",
    "dummy_data = pd.read_csv(dummy_data_dataset_file, encoding='utf-8')\n",
    "dummy_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the dummy_data file there are 3 different types of entries. We will create an individual dataframe for each one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# saving the three types of text data in 3 separate dataframes\n",
    "sms_df = dummy_data[dummy_data['type'] == \"sms\"]\n",
    "review_df = dummy_data[dummy_data['type'] == \"review\"]\n",
    "news_df = dummy_data[dummy_data['type'] == \"news_article\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sample entries on which we can test preprocessing methods\n",
    "\n",
    "sms_sample = \"\"\"***** CONGRATlations **** You won 2 tIckETs to Hamilton in \n",
    "NYC http://www.hamiltonbroadway.com/J?NaIOl/event   wORtH over $500.00...CALL \n",
    "555-477-8914 or send message to: hamilton@freetix.com to get ticket !! !\"\"\"\n",
    "review_sample = \"\"\" THIS FOOD AND STAFF WAS AMAZING!!!!! ABSOLUTELY LOVE THAT PLACE <3<3<3\"\"\"\n",
    "news_sample = \"\"\"worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (┬ú5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Removing unwanted characters\n",
    "The is a primary step in the process of text cleaning. If we scrap some text from HTML/XML sources, we’ll need to get rid of all the tags, HTML entities, punctuation, non-alphabets, and any other kind of characters that might not be a part of the language. The general methods of such cleaning involve **regular expressions**, which can be used to filter out most of the unwanted texts.\n",
    "\n",
    "However, sometimes, depending on the type of data, we want to retain certain types of punctuation. Consider for example human-generated tweets which you want to classify as very angry, angry, neutral, happy, and very happy. Simple sentiment analysis might find it hard to differentiate between a happy, and very happy sentiment because the only difference between a happy and a very happy tweet might be punctuation.\n",
    "\n",
    "Example:\n",
    "\n",
    "*This is amazing* vs *THIS IS AMAZING!!!!!*\n",
    "\n",
    "Or what about this one\n",
    "\n",
    "*I don't know :) <3* vs *I don't know :(((*\n",
    "\n",
    "Now let's create a simple function that keeps only letters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#regular expression keeping only letters \n",
    "\n",
    "def keep_letters_only(raw_text):\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    return letters_only_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keep_letters_only(sms_sample)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see that this is not ideal as this leaves us with a lot of random stuff like \"www\" and \"com\". We will get back to that later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keep_letters_only(review_sample) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We don't lose any meaning, but as mentioned previously, keeping the exclamation marks might be useful if we want to distinguish between positive and *VERY* positive reviews."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keep_letters_only(news_sample)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For news articles that works perfectly fine as we do not lose any relevant information in this case since we want to classify by genre (sports, business, tech, etc.)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Text Normalisation\n",
    "Recall our sms sample:\n",
    "\n",
    "**** **** CONGRATlations **** You won 2 tIckETs to Hamilton in \n",
    "NYC http://www.hamiltonbroadway.com/J?NaIOl/event   wORtH over $500.00...CALL \n",
    "555-477-8914 or send message to: hamilton@freetix.com to get ticket !! !\n",
    "\n",
    "I'd definitely deem this as spam. But clearly, there's a lot going on here: phone numbers, emails, website URLs, money amounts, and gratuitous whitespace and punctuation. Some terms are randomly capitalized, others are in all-caps. Since these terms might show up in any one of the training examples in countless forms, we need a way to ensure each training example is on an equal footing via a preprocessing step called **normalization**. \n",
    "\n",
    "To detect spam messages we don't want the computer to know or remember which email address or phone number was previously used in a spam message. We want the computer to understand **the pattern** of a spammy message. For example, if the message contains a lot of money amounts, words like \"congratulations\", \"you won\", AND an email address, it should be more likely to be considered spam. Again, we do not care what was the particular email address.\n",
    "\n",
    "So instead of removing the following terms, for each training example, let's replace them with a specific string.\n",
    "\n",
    "- Replace email addresses with `emailaddr`\n",
    "- Replace URLs with `httpaddr`\n",
    "- Replace money symbols with `moneysymb`\n",
    "- Replace phone numbers with `phonenumbr`\n",
    "- Replace numbers with `numbr`\n",
    "- get rid of all other punctuations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normalisation_sms(raw_text):\n",
    "    cleaned = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', raw_text)\n",
    "    cleaned = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr',\n",
    "                     cleaned)\n",
    "    cleaned = re.sub(r'£|\\$|\\€', 'moneysymb ', cleaned) #add whitespace\n",
    "    cleaned = re.sub(\n",
    "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "        'phonenumbr', cleaned)\n",
    "    cleaned = re.sub(r'\\d+(\\.\\d+)?', 'numbr', cleaned)\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", cleaned)\n",
    "    return letters_only_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "normalisation_sms(sms_sample)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Tokenisation\n",
    "Tokenisation is the process of splitting a sentence into words (tokens).\n",
    "\n",
    "As you remember, in the previous notebook we used the `.split()` method which may be helpful in this case. Let's see an easy example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"A bad day in London is still better than a bad day anywhere else\".split())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, this sentence has been broken down to 14 tokens of 12 unique types (token 'A' is not the same as token 'a'). \n",
    "\n",
    "This example divides the individual entities but doesn't get rid of the capitalism involved (no pun intended). Capitalization and De-capitalisation are again, dependent on the data and the task at hand.\n",
    "\n",
    "In this case, it seems reasonable to de-capitalize text. Converting the uppercase 'A' to lowercase is a good idea since it has the same meaning as 'a'. Let's do it!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"A bad day in London is still better than a bad day anywhere else\".lower().split())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, so we have changed all characters to lowercase but is it better now? Look what happened to \"London\" - its first letter changed as well. This is a good example of when de-capitalization may not be the best solution. Imagine that there exists an item called \"london\". Because of this, the NLP algorithm developed further may confuse the city of London with the item \"london\". \n",
    "\n",
    "So if we want to differentiate between any sentiments, then something written in uppercase might mean something different than something written in lowercase. \n",
    "\n",
    "Note that in the example above there was no punctuation. Let's see what happens in the following case."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"A bad day in London, is still better than a bad day anywhere else! London is the capital of the UK.\".lower().split())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "'London' and 'London,' are not the same thing! Of course, people are smart enough to understand that both tokens have the same meaning. However, computer algorithms looking for *patterns* may treat these two tokes as totally different and unrelated things.\n",
    "\n",
    "The simplest solution would be to remove all the punctuation but as we said earlier, this may lead to the loss of meaning/sentiment. Is there any clever way we can solve this issue and keep punctuation? Yes! We can use regular expressions to match the word boundaries and treat punctuation as separate tokens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(re.findall(r'\\b\\w+\\b|[^\\w ]', \"A bad day in London, is still better than a bad day anywhere else! London is the capital of the UK.\".lower()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cool, this seems to work. How about this one?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(re.findall(r'\\b\\w+\\b|[^\\w ]', \"Although I do like rain, I don't like this stormy weather!\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Look what happened to \"don't\". It has been treated as two words split with the apostrophe. Since \"don't\" is the negation of \"do\", it would be natural to split \"don't\" somehow differently, showing that fact. So let's create a rule, that every \"don't\" will be split to \"do\" and \"n't\". But what with \"can't\" or \"shouldn't\"? Or even totally different words containing apostrophes like \"students'\"?  \n",
    "\n",
    "As you can see, we have to develop rules for multiple cases - quite boring and time-consuming. This is why we introduce another Python module called `nltk` - Natural Language Toolkit. This module contains many useful text processing tools including tokenizers. Let's see how it works."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(\"I don't like stormy weather after 8 o'clock in the evening!\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see `word_tokenizer` does exactly what we want! Nltk provides also different tokenizers for different types of input. Let's compare the `word_tokenize` with `TweetTokenizer`, which has been designed to work better with Twitter-type source texts (including hashtags, mentions, etc.)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "print(word_tokenize(\"Hey @everyone, this is a sample #Twitter text containing some emojis :))) !!! Have fun <3 !\"))\n",
    "print(tt.tokenize(\"Hey @everyone, this is a sample #Twitter text containing some emojis :))) !!! Have fun <3 !\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 Stopword removal\n",
    "\n",
    "Stopwords are the words that are used very frequently. Words like “of, are, the, it, is” are some examples of stopwords. In applications like document search engines and document classification, where keywords are more important than general terms, removing stopwords can be a good idea. However, if there’s some application about, for instance, songs lyrics search, or searching for specific quotes, stopwords can be important. \n",
    "\n",
    "“To be, or not to be” - Stopwords in such phrases actually play an important role, and hence, should not be dropped.\n",
    "\n",
    "Another example is negation. \"not\" is contained in many stopword lists, but deleting \"not\" out of a negative review can make a positive out of it.\n",
    "\n",
    "There are two common approaches to removing the stopwords, and both are fairly straightforward. One way is to count all the word occurrences, and providing a threshold value on the count, and getting rid of all the terms/words occurring more than the specified threshold value. The other way is to have a predetermined list of stopwords, which can be removed from the list of tokens/tokenized sentences. In the beginning, the second one may be better, as determining thresholds can be quite difficult.\n",
    "\n",
    "NLTK comes with many corpora, including a stopword list. This list contains around 200 terms. However, you may want to use one that contains over 600 terms: [http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop](http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop) (Apostrophes have been removed as it has been done for the news articles)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, let's see how we can remove stopwords from the news article sample"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Firstly, let's read the stopwords file\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "''' uncomment if you are running this notebook locally '''\n",
    "# with open(\"./datasets/SmartStoplist.txt\", 'r') as f:\n",
    "#     stop_words.extend(f.read().splitlines())\n",
    "\n",
    "''' uncomment if you are using google colab '''\n",
    "import urllib\n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook2/datasets/SmartStoplist.txt\") as f:\n",
    "    stop_words.extend(f.read().decode('utf-8').splitlines())\n",
    "\n",
    "    \n",
    "print(stop_words[:10])  # First 10 stopwords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We can remove stopwords using our stopwords list, or...\n",
    "nltk_tokens = word_tokenize(news_sample)\n",
    "filtered_sentence_smart = [w for w in nltk_tokens if not w in stop_words]\n",
    "\n",
    "\n",
    "# ...again, we can use nltk builtin stopwords feature\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "filtered_sentence_nltk = [w for w in nltk_tokens if not w in stop_words_nltk]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(news_sample)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(filtered_sentence_nltk)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(filtered_sentence_smart)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.6 Lemmatising and Stemming\n",
    "Lemmatisation and stemming both refer to a process of reducing a word to its root. The difference is that stem might not be an actual word whereas, a lemma is an actual word. It’s a handy tool if you want to avoid treating different forms of the same word as different words, e.g. *love, loved, loving*\n",
    "\n",
    "**Lemmatising:** considered, considers, consider → “consider”\n",
    "\n",
    "**Stemming:** considered, considering, consider → “consid”\n",
    "\n",
    "In many applications, there may be no significant difference between lemmatising and stemming when training classifiers. However, the best way to find out how they work and when to use which solution is to try them! NLTK comes with many different in-built lemmatisers and stemmers, so just plug and play.\n",
    "\n",
    "A note of caution: WordNetLemmatizer requires a POS-tag. The default is set to \"noun\" and therefore doesn't work with other words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word = \"considers\"\n",
    "word_2 = \"apple\"\n",
    "\n",
    "stemmed_word =  stemmer.stem(word)\n",
    "lemmatised_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "stemmed_word_2 =  stemmer.stem(word_2)\n",
    "lemmatised_word_2 = lemmatizer.lemmatize(word_2)\n",
    "\n",
    "print(stemmed_word)\n",
    "print(lemmatised_word)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(stemmed_word_2)\n",
    "print(lemmatised_word_2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.7 Putting it all together\n",
    "Now that we covered everything we need to know, we can combine everything into one function and apply it to the whole data. Let's keep it simple and write one for the news articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess_news(raw_text):\n",
    "    \n",
    "    #keeping only letters\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and tokenise\n",
    "    tokens = word_tokenize(letters_only_text.lower())\n",
    "    \n",
    "\n",
    "    cleaned_words = []\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # remove stopwords\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    # stemm or lemmatise words\n",
    "    stemmed_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = stemmer.stem(word)\n",
    "        stemmed_words.append(word)\n",
    "    \n",
    "    # converting list back to string\n",
    "    return \" \".join(stemmed_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "news_sample"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preprocess_news(news_sample)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "news_df['prep_text'] = news_df['text'].apply(preprocess_news)\n",
    "news_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Part-of-speech tagging and Named-entity recognition\n",
    "\n",
    "One may be interested not only in lexical features or pattern based features like punctuation or upper/lower case letters but also in semantic features in the source. This may be for example detecting verbs or searching for location names (\"London\", \"New York\", etc.).\n",
    "- **POS tagging** - determining the lexical type of a given token (verb, noun, etc.) \n",
    "- **NER** - identifying and classifying named-entities into general groups (person name, money amount, time)\n",
    "\n",
    "To understand both tasks let's look at the simple example. Let's say we have a following sentence \"John visited US in 2020.\".\n",
    "\n",
    "| Operation | Output |\n",
    "|-----------|--------|\n",
    "| Raw text | John   visited   US   in   2020  . |\n",
    "| POS tags | John<sub>proper noun</sub>   visited<sub>verb</sub>   US<sub>proper noun</sub>   in<sub>adposition</sub>   2020<sub>number</sub>  .<sub>punctuation</sub> |\n",
    "| NER | John<sub>Person</sub>   visited   US<sub>Country</sub>   in   2020<sub>Time</sub>  . |\n",
    "\n",
    "Now, let's try to do the same using another Python module called `spaCy`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to download spacy english language package\n",
    "! python3 -m spacy download en_core_web_sm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\n",
    "\n",
    "# Firstly, we need to load the English language model\n",
    "model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence = \"John visited US in 2020.\"\n",
    "doc = model(sentence)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's see how does `spacy` tags these words with part-of-speech types:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we would like to see those types and relations between word visually, `spacy` comes with a very handy method for doing so."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, jupyter=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, how about named-entities? How can we identify and tag them?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also visualize those tags!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "# GPE stands for the Geopolitical entity"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Spacy` gives many tools for this kind of analysis and identification. It also contains preprocessing tools like tokenizers and even advanced word embeddings. Make sure to check its [documentation](https://spacy.io/usage#quickstart)!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Bag-of-words model\n",
    "Ok, now that we have tokenised and preprocessed our text, it is time to convert it into computer-readable vectors. This is called feature extraction. The **bag-of-words (BOW) model** is a popular and simple feature extraction technique. The intuition behind BOW is that two sentences are said to be similar if they contain a similar set of words. Bag-of-words can be treated as a special case of a more complex, **n-gram language model**.\n",
    "\n",
    "The general idea of the BOW model is to count how many times each word (*token*) from the dataset occurs in a given sentence/source. The simplest way of implementing this model is using Python dictionaries. Let's try!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sentence1 = \"They like apples\"\n",
    "sentence2 = \"We like bananas\"\n",
    "\n",
    "sentence1_bag = {}\n",
    "sentence2_bag = {}\n",
    "\n",
    "def create_bag(text):\n",
    "    bag = {}\n",
    "    for token in text.split():\n",
    "        if token in bag:\n",
    "            bag[token] += 1\n",
    "        else:\n",
    "            bag[token] = 1\n",
    "    return bag\n",
    "\n",
    "sentence1_bag = create_bag(sentence1)\n",
    "sentence2_bag = create_bag(sentence2)\n",
    "print(sentence1_bag)\n",
    "print(sentence2_bag)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, now the computer understands how many and which words make up each sentence but is it able to compare them? No, because there isn't any connection between those sentences (yet!). We have to develop a \"common denominator\" for both sentences so we can compare them. \n",
    "\n",
    "### 5.1 One-Hot Vectors\n",
    "In the case of the BOW model, the solution is to create a *bag* of all used tokens and encode words using computer-readable **One-Hot Vectors**. How does it work? BOW constructs a dictionary of *m* unique words in the corpus (vocabulary) and converts each word into a sparse vector of size *m*, where all values are set to 0 apart from the index of that word in the vocabulary. We can also say that each word is a feature and that sentences consist of features. If a feature is present in a given sentence it means one thing, if it is not present it means something different."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the case above there are five different words: \"They\", \"We\", \"like\", \"apples\", \"bananas\". We can encode them using a vector of length 5.\n",
    "\n",
    "| word    | associated vector |\n",
    "|---------|-------------------|\n",
    "| They    | [1,0,0,0,0]       |\n",
    "| We      | [0,1,0,0,0]       |\n",
    "| like    | [0,0,1,0,0]       |\n",
    "| apples  | [0,0,0,1,0]       |\n",
    "| bananas | [0,0,0,0,1]       |\n",
    "\n",
    "\n",
    "A sentence can be represented by adding the vectors together.\n",
    "\n",
    "For example: *They like apples* can be expressed as *They + like + apples* and using vectors: [1,0,0,0,0] + [0,0,1,0,0] + [0,0,0,1,0] = [1,0,1,1,0], hence the computer readable version of \"They like apples\" is [1,0,1,1,0]. Simple, right? Unfortunately, it is often too simple - look what happens if you transfrm \"apples like They\" to vectors: the result is exactly the same [1,0,1,1,0]! We can use the n-gram model to eliminate this issue.\n",
    "\n",
    "| sentence           | associated sum of vectors |\n",
    "|--------------------|---------------------------|\n",
    "| They like apples   | [1,0,1,1,0]               |\n",
    "| We like apples     | [0,1,1,1,0]               |\n",
    "| We like bananas    | [0,1,1,0,1]               |\n",
    "| apples like They   | [1,0,1,1,0]               |\n",
    "\n",
    "What if there are more than 1 occurrence of the same token? There are different ways of handling that: max-pooling only counts whether a word is present, but not how many times. Sum pooling counts the number of occurrences of each word.\n",
    "\n",
    "| sentence                     | method            | associated sum of vectors |\n",
    "|------------------------------|-------------------|---------------------------|\n",
    "| They like like like apples   | max-pooling       |[1,0,1,1,0]                |\n",
    "| They like like like apples   | sum pooling       |[1,0,3,1,0]                |\n",
    "\n",
    "\n",
    "\n",
    "Now, how to implement it in Python? Of course, we could develop our own methods of words vectorization but the `scikit-learn` package gives us a set of useful tools!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Corpus containing all sentences\n",
    "corpus = [sentence1, sentence2]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus)  # vectorizer learns numbers of word occurrences (features)\n",
    "print(vectorizer.get_feature_names())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, count vectorizer created a set of 5 features based on which it will \"score\" given sentences. Let's see how to transform a sentence into a corresponding vector."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(sentence1)\n",
    "print(vectorizer.transform([sentence1]).toarray())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(sentence2)\n",
    "print(vectorizer.transform([sentence2]).toarray())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"like like like\")\n",
    "print(vectorizer.transform([\"like like like\"]).toarray())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see CountVectorizer is sum pooling by default. If you want to change it to max-pooling add a parameter \"binary=True\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectorizer.fit(corpus)\n",
    "print(\"like like like\")\n",
    "print(vectorizer.transform([\"like like like\"]).toarray())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 BOW model limitations - the word order\n",
    "Although bag-of-words is simple, easy to implement and in some applications works quite well, it has some serious limitations. The first one is the one we have already discussed - the word order does not matter. If the purpose of the model is to classify texts based on some keywords then it may be not important what was the order of words. However, it's quite tricky - two single words may have very different meaning when they occur together. Let's take a closer look at this example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's say there are two reviews of the same movie \n",
    "corpus = [\"The movie was not bad, actually quite good!\", \"The movie was not good, actually quite bad!\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus)  # vectorizes learns numbers of word occurences (features)\n",
    "print(vectorizer.get_feature_names())\n",
    "for sentence in corpus:\n",
    "    print(sentence)\n",
    "    print(vectorizer.transform([sentence]).toarray())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although those reviews express completely opposite emotions, both senteces have been represented in the exactly same way. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3 BOW model limitations - previously unseen words \n",
    "The next problem is new words. What happens if we ask our model to vectorize a text which contains previously unseen words (i.e. those words weren't present in the training corpus)? We cannot add them to the corpus and encode them on the fly since this will change the length of the vector. The only reasonable solution is to dismiss all words which were not in the training corpus. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus = [\"They like bananas\", \"We like apples\"]\n",
    "\n",
    "test_sentence1 = \"We like apples, bananas\"\n",
    "test_sentence2 = \"We like apples, bananas, plums\"\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus)  # vectorizes learns numbers of word occurrences (features)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(test_sentence1)\n",
    "print(vectorizer.transform([test_sentence1]).toarray())\n",
    "print(test_sentence2)\n",
    "print(vectorizer.transform([test_sentence2]).toarray())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, both sentences got encoded to the same vector. Because of this, we lost the additional information from the second sentence that \"they\" also like plums. The solution for this problem is to use sufficiently large training corpora but this leads to a significant performance drop and unreasonable memory usage (for corpus containing 100k words, every sentence is represented using a vector of length 100k). I encourage you to read [this chapter](https://web.stanford.edu/~jurafsky/slp3/3.pdf) of the SLP book about a better model - the **n-gram language model**.\n",
    "\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}