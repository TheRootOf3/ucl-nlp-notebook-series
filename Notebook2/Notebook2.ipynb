{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Load from csv to pandas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "dummy_ds = pd.read_csv('dummy_data.csv', encoding='utf-8')\n",
    "dummy_ds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the dummy_data file there are 3 different types of entries. We would like to use only those of type `sms`, so we have to create a separate DataFrame."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sms_df = dummy_ds[dummy_ds['type'] == 'sms']\n",
    "sms_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# let's create a sample entry on which we can test preprocessing methods\n",
    "\n",
    "sms_sample = \"\"\"***** CONGRATlations **** You won 2 tIckETs to Hamilton in \n",
    "NYC http://www.hamiltonbroadway.com/J?NaIOl/event   wORtH over $500.00...CALL \n",
    "555-477-8914 or send message to: hamilton@freetix.com to get ticket !! !\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing unwanted chars\n",
    "The is a primary step in the process of text cleaning. If we scrap some text from HTML/XML sources, weâ€™ll need to get rid of all the tags, HTML entities, punctuation, non-alphabets, and any other kind of characters which might not be a part of the language. The general methods of such cleaning involve regular expressions, which can be used to filter out most of the unwanted texts.\n",
    "\n",
    "However, sometimes, depending on the type of data, we want to retain certain types of punctuation. Consider for example human generated tweets which you want to classify as very angry, angry, neutral, happy, and very happy. Simple sentiment analysis might find it hard to differentiate between a happy, and a very happy sentiment, because the only difference between a happy and a very happy tweet might be punctuation.\n",
    "\n",
    "Example:\n",
    "\n",
    "*This is amazing*    <pre>  vs     </pre>  *THIS IS AMAZING!!!!!*\n",
    "\n",
    "---\n",
    "\n",
    "Or what about this one\n",
    "\n",
    "\n",
    "\n",
    "*I don't know :) <3* <pre>  vs     </pre>  *I don't know :(((*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stopword removal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lemmatising and Stemming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All together"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Language models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bag of words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### N-gram"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}